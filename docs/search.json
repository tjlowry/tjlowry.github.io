[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Data Science Blog",
    "section": "",
    "text": "Organizing Streamlit Dashboards with Functions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBoosting Forecast Accuracy Through Model Ensembling\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime Series Forecasting: Understanding the Components\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime Series Forecasting: ARIMA and ETS Models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tyler Lowry",
    "section": "",
    "text": "I’m a Data Science student at BYU-Idaho passionate about turning complex data into actionable insights. With a minor in Accounting and a Machine Learning Certificate, I hope to bridge the gap between business and statistics.\nAt BYU-Idaho, I serve as both a TA/Tutor in the Mathematics Department and Team Lead at the McKay Maclab, which has strengthened my skills in programming, statistics, and leadership.\nAfter graduating in April 2025, I’ll be continuing my education at Texas A&M for a Master’s in Statistical Data Science.\n\n\n\n\n\n\n\n\n\n\n\nPython (Pandas, Polars, Scikit-learn, TensorFlow)\nR\nSQL\n\n\n\n\n\nTableau\nPower BI\nggplot\nPlotly, lets_plot, Seaborn\n\n\n\n\n\n\nSQL Database Optimization\nAWS EC2/S3\nDatabase Design & Architecture\n\n\n\n\n\nPHP (Filament), C#\nJavaScript/TypeScript\nHTML/CSS\n\n\n\n\n\n\n\n\nCheck out my Projects page for a showcase of my recent work"
  },
  {
    "objectID": "index.html#skills",
    "href": "index.html#skills",
    "title": "Tyler Lowry",
    "section": "",
    "text": "Python (Pandas, Polars, Scikit-learn, TensorFlow)\nR\nSQL\n\n\n\n\n\nTableau\nPower BI\nggplot\nPlotly, lets_plot, Seaborn\n\n\n\n\n\n\nSQL Database Optimization\nAWS EC2/S3\nDatabase Design & Architecture\n\n\n\n\n\nPHP (Filament), C#\nJavaScript/TypeScript\nHTML/CSS"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "tjlowry02@gmail.com | LinkedIn"
  },
  {
    "objectID": "resume.html#education",
    "href": "resume.html#education",
    "title": "Resume",
    "section": "Education",
    "text": "Education\nBrigham Young University-Idaho | Rexburg, ID\nBS, Data Science - Accounting Minor - Machine Learning Certificate\nExpected Graduation: April 2025\nGPA: 3.89"
  },
  {
    "objectID": "resume.html#skills",
    "href": "resume.html#skills",
    "title": "Resume",
    "section": "",
    "text": "R, Python, Pandas, Polars, SQL\nMachine Learning, Demand Forecasting, TensorFlow, Scikit-Learn\nTableau, Power BI\n\n\n\n\n\nHTML, CSS, JavaScript, PHP, PySpark\nAPIs, Data Pipelines\nExcel, VBA, Adobe Suite"
  },
  {
    "objectID": "resume.html#professional-experience",
    "href": "resume.html#professional-experience",
    "title": "Resume",
    "section": "Professional Experience",
    "text": "Professional Experience\n\nMachine Learning Engineer Intern | PIC Business Systems\nMay 2023 - Sept 2023 | San Antonio, TX\n\nDeveloped a 90-day sales forecasting ensembled model and an interactive dashboard for product performance, resulting in a 30% improvement in median forecast MAPE\nEnhanced data quality by integrating external factors such as weather and date, using APIs and Pandas to create additional features for the model\nAutomated monthly forecasting and database updates by leveraging AWS Lambda and S3 Buckets, enabling easy access to forecasts and reducing manual efforts\n\n\n\nTA/Tutor | BYU-Idaho Department of Mathematics\nApril 2024 - Present | Rexburg, ID\n\nServed as a teaching assistant and tutor for “Intro to Programming,” “Math for the Real World,” “Data Wrangling and Visualization,” “Data Science Programming,” and “Biostatistics”\nLed group and one-on-one tutoring sessions focused on Python, R, Excel, and statistics\nProvided general data science support at the Data Science Tutoring Lab, assisting students across various data science and statistics courses with Python, R, and other coursework-related questions\n\n\n\nTeam Lead/Developer | BYU-Idaho McKay Maclab\nJan 2023 - Present | Rexburg, ID\n\nCollaborated with a team of 3 in full stack development of Maclab website and 3D printing queue\nConducted interviews, reviewed resumes to screen candidates, and provided training to new employees\nFacilitated weekly meetings to discuss goals, track task progress, and deliver short training sessions to the team\nDesigned interactive streamlit dashboards to track KPI performance, resulting in a 14% average improvement across all metrics\n\n\n\nIntern | George W. Lowry Inc.\nSept 2023 - Dec 2023 | Salida, CA\n\nMigrated legacy database system to AWS and optimized database architecture by eliminating redundant joins, improving foreign key usage, and refining indexes, resulting in a nearly two-hour reduction in report generation runtime\nIdentified and implemented an AWS EC2 instance, increasing computing power while lowering costs, reducing server crashes, and improving efficiency\nDeveloped a CRM using a PHP framework, enhancing location data tracking and streamlining communication between sales team and delivery drivers for improved logistics"
  },
  {
    "objectID": "posts/2024-01-16-machine-learning-forecast/index.html",
    "href": "posts/2024-01-16-machine-learning-forecast/index.html",
    "title": "Building a Sales Forecasting Model",
    "section": "",
    "text": "During my internship at PIC Business Systems, I had the opportunity to develop a sales forecasting model that improved prediction accuracy by 30%. Here’s how I approached it."
  },
  {
    "objectID": "posts/2024-01-16-machine-learning-forecast/index.html#the-challenge",
    "href": "posts/2024-01-16-machine-learning-forecast/index.html#the-challenge",
    "title": "Building a Sales Forecasting Model",
    "section": "The Challenge",
    "text": "The Challenge\nAccurately predicting sales for the next 90 days was crucial for inventory management and business planning. The existing system relied heavily on manual predictions and didn’t account for external factors."
  },
  {
    "objectID": "posts/2024-01-16-machine-learning-forecast/index.html#the-solution",
    "href": "posts/2024-01-16-machine-learning-forecast/index.html#the-solution",
    "title": "Building a Sales Forecasting Model",
    "section": "The Solution",
    "text": "The Solution\nI developed an ensemble model that combined: - XGBoost - Prophet - SARIMA\nThe model incorporated external factors like: - Weather patterns - Seasonal trends - Historical sales data - Market indicators\n[Continue reading for more technical details…]"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Projects",
    "section": "",
    "text": "My Projects\nThis page will be updated soon with projects I have worked on."
  },
  {
    "objectID": "about.html#what-im-passionate-about",
    "href": "about.html#what-im-passionate-about",
    "title": "About Me",
    "section": "",
    "text": "As a Machine Learning Engineer and Developer, I love tackling challenges that combine data science with practical applications. Whether it’s developing sales forecasting models or optimizing database performance, I enjoy seeing how data can drive real business impact."
  },
  {
    "objectID": "about.html#current-role",
    "href": "about.html#current-role",
    "title": "About Me",
    "section": "",
    "text": "At BYU-Idaho, I serve as both a TA/Tutor in the Mathematics Department and Team Lead at the McKay Maclab. I get to combine my technical skills with leadership, whether I’m helping students grasp programming concepts or leading development projects."
  },
  {
    "objectID": "about.html#beyond-the-code",
    "href": "about.html#beyond-the-code",
    "title": "About Me",
    "section": "",
    "text": "When I am not working with data, I love to spend time with my wife and our adopted cat George, a stray we found as a kitten in a California warehouse. I am a huge sports fan and root for all the Arizona-based sports teams even though I’m from San Antonio, TX. I became a fan through my mom, who grew up in Arizona. Some other hobbies of mine are 3d printing, history, and sci-fi."
  },
  {
    "objectID": "about.html#connect-me",
    "href": "about.html#connect-me",
    "title": "About Me",
    "section": "",
    "text": "Connect with me on LinkedIn!"
  },
  {
    "objectID": "posts/2025-02-21-ts-forecast/index.html",
    "href": "posts/2025-02-21-ts-forecast/index.html",
    "title": "Time Series Forecasting: ARIMA and ETS Models",
    "section": "",
    "text": "A stationary time series has consistent statistical properties over time - constant mean, variance, and autocorrelation structure. Most forecasting models require stationarity to work effectively.\nSigns of non-stationarity include: - Visible trend (upward/downward) - Changing variance over time - Seasonal patterns\n\n\n\nDifferencing transforms non-stationary data into stationary data by subtracting consecutive observations: - First-order differencing: y’ₜ = yₜ - yₜ₋₁ - Second-order differencing: y’’ₜ = y’ₜ - y’ₜ₋₁\nDifferent levels of differencing are used depending on how persistent trends are in your data, but the goal is to flatten out the trend in the data.\n\n\n\nARIMA(p,d,q) combines three components: - p (AutoRegressive): Number of lag observations used - d (Integrated): Number of differencing required to make stationary - q (Moving Average): Size of moving average window\nFor example, ARIMA(1,1,1) uses 1 lag value, applies first differencing, and includes 1 moving average term.\nSeasonal patterns can be handled with SARIMA, adding seasonal parameters (P,D,Q,m) where m is the seasonality period.\n\n\n\nETS stands for Error, Trend, Seasonality, focusing on the components we decompose time series into: - Error: Additive (A) or Multiplicative (M) - Trend: None (N), Additive (A), or Multiplicative (M), with optional dampening - Seasonality: None (N), Additive (A), or Multiplicative (M)\nFor example, ETS(A,A,N) handles data with additive error, additive trend, and no seasonality.\n\n\n\nAuto ARIMA algorithms automatically search for optimal ARIMA parameters by: 1. Testing different combinations of p, d, q values 2. Using information criteria (AIC, BIC) to select the best model 3. Testing for stationarity and determining proper differencing\nfrom pmdarima import auto_arima\n\nmodel = auto_arima(data, seasonal=True, m=12,\n                  start_p=0, max_p=3, start_q=0, max_q=3,\n                  d=None, max_d=2, trace=True)\n\nseasonal=True: Tells the function to look for seasonal patterns in the data\nm=12: Sets the seasonal period to 12 (good for monthly data)\nstart_p=0, max_p=3: Tests AR terms from 0 to 3\nstart_q=0, max_q=3: Tests MA terms from 0 to 3\nd=None, max_d=2: Lets the function automatically determine differencing up to order 2\ntrace=True: Prints the model results as they’re being tested\n\nAutoArima will go through possible combinations of these parameters in a step-wise method finding the combination that best minimizes AIC and BIC\n\n\n\nUse ARIMA when: - Data shows complex autocorrelation patterns - You want to model time series based on previous values and errors, use the past to explain the present - Data becomes stationary after differencing\nUse ETS when: - You have clear trend and seasonal patterns - You have limited historical data - Want the trend modeled into the data\nFor adding a resources section at the end of your blog post, here’s a simple and effective approach:\n\n\n\n\nOnline Resources:\n\nAutoArima Documentation\nTime series textbook\nStatsmodel ETS Documentation"
  },
  {
    "objectID": "posts/2025-02-21-ts-forecast/index.html#why-both-r-and-python",
    "href": "posts/2025-02-21-ts-forecast/index.html#why-both-r-and-python",
    "title": "Time Series Forecasting: First Models in R and Python",
    "section": "",
    "text": "While Python has become my go-to language for data science work, I feel R is more intutive for certain aspects of time series forecasting due to the great packages which have been devolped. Each has distinct advantages:\n\nR excels with its purpose-built statistical packages and elegant handling of time series objects\nPython offers incredible flexibility, integration with other systems, and powerful machine learning capabilities\n\nLearning both approaches gives you a more complete toolkit and the ability to choose the right tool for each specific forecasting challenge."
  },
  {
    "objectID": "posts/2025-02-21-ts-forecast/index.html#getting-started-with-time-series-data",
    "href": "posts/2025-02-21-ts-forecast/index.html#getting-started-with-time-series-data",
    "title": "Time Series Forecasting: First Models in R and Python",
    "section": "",
    "text": "Before diving into models, we need data. For this example, let’s use a classic time series dataset: monthly airline passenger numbers. This dataset shows a clear upward trend and strong seasonality, making it perfect for demonstrating basic forecasting techniques."
  },
  {
    "objectID": "posts/2025-02-21-ts-forecast/index.html#first-models-in-r",
    "href": "posts/2025-02-21-ts-forecast/index.html#first-models-in-r",
    "title": "Time Series Forecasting: First Models in R and Python",
    "section": "",
    "text": "R has specialized libraries designed specifically for time series analysis. The forecast package, created by Rob Hyndman, is particularly powerful and user-friendly.\n\n\n# Install and load necessary packages\ninstall.packages(c(\"forecast\", \"tseries\"))\nlibrary(forecast)\nlibrary(tseries)\nlibrary(ggplot2)\n\n# Load the airlines dataset (included in R)\ndata(\"AirPassengers\")\nair &lt;- AirPassengers  # Monthly totals of airline passengers from 1949-1960\n\n# Examine the data\nclass(air)  # Should be \"ts\" (time series)\nfrequency(air)  # 12 = monthly data\nstart(air)  # Starting time period\nend(air)  # Ending time period\n\n# Plot the data\nautoplot(air) + \n  ggtitle(\"Monthly Airline Passenger Numbers 1949-1960\") + \n  xlab(\"Year\") + \n  ylab(\"Passengers (thousands)\")\n\n# Decompose the time series\nair_decomp &lt;- decompose(air, type = \"multiplicative\")\nautoplot(air_decomp)\n\n# Check stationarity with Augmented Dickey-Fuller test\nadf.test(air)  # p-value &gt; 0.05 indicates non-stationarity\n\n# Differencing to achieve stationarity\nair_diff &lt;- diff(log(air))\nadf.test(air_diff)  # Should now be stationary\n\n# Examine ACF and PACF for model parameter selection\nacf(air_diff, main=\"ACF of Differenced Log Passenger Data\")\npacf(air_diff, main=\"PACF of Differenced Log Passenger Data\")\n\n# Fit an ARIMA model automatically\nair_model &lt;- auto.arima(air, seasonal = TRUE)\nsummary(air_model)\n\n# Forecast the next 24 months\nair_forecast &lt;- forecast(air_model, h = 24)\n\n# Plot the forecast\nautoplot(air_forecast) + \n  ggtitle(\"24-Month Forecast of Airline Passengers\")\nThis R example walks through the complete process: 1. Loading and examining the data 2. Visualizing and decomposing the time series 3. Testing for stationarity (crucial for ARIMA models) 4. Using auto.arima() to automatically select the best parameters 5. Generating and visualizing a forecast\nThe auto.arima() function automatically determines the appropriate ARIMA model parameters (p, d, q) and their seasonal counterparts (P, D, Q), saving us from manual parameter tuning."
  },
  {
    "objectID": "posts/2025-02-21-ts-forecast/index.html#first-models-in-python",
    "href": "posts/2025-02-21-ts-forecast/index.html#first-models-in-python",
    "title": "Time Series Forecasting: First Models in R and Python",
    "section": "",
    "text": "Python offers multiple libraries for time series forecasting. The statsmodels package provides statistical models while pandas handles the data manipulation.\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom pandas.plotting import register_matplotlib_converters\nregister_matplotlib_converters()\n\n# Load the AirPassengers dataset\nair = pd.read_csv('https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv',\n                  index_col='Month', parse_dates=True)\nair.columns = ['Passengers']\n\n# Plot the data\nplt.figure(figsize=(10, 6))\nplt.plot(air)\nplt.title('Monthly Airline Passenger Numbers 1949-1960')\nplt.xlabel('Year')\nplt.ylabel('Passengers (thousands)')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# Decompose the time series\ndecomposition = seasonal_decompose(air, model='multiplicative')\nfig = decomposition.plot()\nfig.set_size_inches(10, 8)\nplt.tight_layout()\nplt.show()\n\n# Check stationarity with Augmented Dickey-Fuller test\ndef adf_test(series):\n    result = adfuller(series.dropna())\n    print('ADF Statistic: %f' % result[0])\n    print('p-value: %f' % result[1])\n    print('Critical Values:')\n    for key, value in result[4].items():\n        print('\\t%s: %.3f' % (key, value))\n    if result[1] &lt;= 0.05:\n        print(\"Conclusion: The series is stationary\")\n    else:\n        print(\"Conclusion: The series is non-stationary\")\n\nadf_test(air['Passengers'])\n\n# Take log and difference to make stationary\nair['LogPassengers'] = np.log(air['Passengers'])\nair['LogDiffPassengers'] = air['LogPassengers'].diff()\nadf_test(air['LogDiffPassengers'].dropna())\n\n# Plot ACF and PACF for parameter selection\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\nplot_acf(air['LogDiffPassengers'].dropna(), ax=ax1)\nplot_pacf(air['LogDiffPassengers'].dropna(), ax=ax2)\nplt.tight_layout()\nplt.show()\n\n# Fit a SARIMA model (with parameters based on ACF/PACF)\n# For this dataset, a SARIMA(1,1,1)(1,1,1,12) often works well\nmodel = SARIMAX(air['Passengers'], order=(1, 1, 1), seasonal_order=(1, 1, 1, 12))\nresults = model.fit()\nprint(results.summary())\n\n# Forecast the next 24 months\nforecast_steps = 24\nforecast_index = pd.date_range(start=air.index[-1], periods=forecast_steps+1, freq='MS')[1:]\nforecast = results.forecast(forecast_steps)\nforecast_df = pd.DataFrame(forecast, index=forecast_index, columns=['Forecast'])\n\n# Plot the actual data and the forecast\nplt.figure(figsize=(12, 6))\nplt.plot(air['Passengers'], label='Historical data')\nplt.plot(forecast_df, label='Forecast')\nplt.title('24-Month Forecast of Airline Passengers')\nplt.xlabel('Year')\nplt.ylabel('Passengers (thousands)')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# Calculate and plot confidence intervals\npred = results.get_prediction(start=air.index[-1], end=forecast_index[-1])\npred_conf = pred.conf_int()\n\nplt.figure(figsize=(12, 6))\nplt.plot(air['Passengers'], label='Historical data')\nplt.plot(pred.predicted_mean, label='Forecast')\nplt.fill_between(pred_conf.index, \n                 pred_conf.iloc[:, 0], \n                 pred_conf.iloc[:, 1], \n                 color='k', alpha=0.2)\nplt.title('24-Month Forecast with 95% Confidence Interval')\nplt.xlabel('Year')\nplt.ylabel('Passengers (thousands)')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\nThe Python example follows a similar workflow to the R version, but with Python-specific libraries:\n\nLoading data with pandas\nVisualizing with matplotlib\nDecomposing and testing stationarity\nFitting a SARIMA model using statsmodels\nForecasting future values and plotting with confidence intervals"
  },
  {
    "objectID": "posts/2025-02-21-ts-forecast/index.html#key-differences-between-r-and-python-approaches",
    "href": "posts/2025-02-21-ts-forecast/index.html#key-differences-between-r-and-python-approaches",
    "title": "Time Series Forecasting: First Models in R and Python",
    "section": "",
    "text": "As you can see, while the overall process is similar, there are some notable differences:\n\nData Handling: R has native support for time series objects, while Python relies on pandas DateTime indexes\nModel Selection: R’s auto.arima() automatically selects parameters, while in Python we typically need to analyze ACF/PACF plots or use grid search\nVisualization: R’s plotting is more concise with the forecast package, while Python offers more customization with matplotlib\nPerformance: For larger datasets, Python may offer better performance, especially when integrated with other machine learning workflows"
  },
  {
    "objectID": "posts/2025-02-21-ts-forecast/index.html#beyond-arima-next-steps",
    "href": "posts/2025-02-21-ts-forecast/index.html#beyond-arima-next-steps",
    "title": "Time Series Forecasting: First Models in R and Python",
    "section": "",
    "text": "While ARIMA and SARIMA models are great starting points, there are many other approaches to explore:\n\nExponential Smoothing: Simple yet powerful, especially for data with strong seasonality\nProphet: Facebook’s forecasting tool that handles holidays and events effectively\nLSTM Networks: Deep learning approaches for complex sequential patterns\nXGBoost with Lag Features: Machine learning with engineered time features\n\nI’ll cover these more advanced techniques in future posts, showing implementations in both R and Python where appropriate."
  },
  {
    "objectID": "posts/2025-02-21-ts-forecast/index.html#wrapping-up",
    "href": "posts/2025-02-21-ts-forecast/index.html#wrapping-up",
    "title": "Time Series Forecasting: First Models in R and Python",
    "section": "",
    "text": "These examples demonstrate basic forecasting models in R and Python, but they’re just the beginning. The choice between R and Python often comes down to your specific needs and existing workflow:\n\nChoose R if your work is primarily statistical and you value specialized time series tools\nChoose Python if you need integration with larger systems or want to leverage deep learning approaches\n\nIn my experience, learning both gives you the most flexibility. Many data scientists start with R for its statistical focus, then migrate to Python as they integrate forecasting into broader data pipelines and applications.\nIn the next post, I’ll dive deeper into model evaluation and selection - how to measure forecast accuracy and choose the right model for your specific time series problem."
  },
  {
    "objectID": "posts/2025-01-27-machine-learning-forecast/index.html",
    "href": "posts/2025-01-27-machine-learning-forecast/index.html",
    "title": "Time Series Forecasting: Understanding the Components",
    "section": "",
    "text": "Time series forecasting is a fascinating part of data science and machine learning. I first encountered it during a machine learning course where we briefly covered various models and techniques like classification, regression, random forests, LSTMs, CNNs, and gradient boosted models. In that class I was tasked with predicting bike sales. When I mentioned what I was learning to a family member, they asked if these techniques could be applied to demand forecasting in the context of their specific business. I thought it couldn’t be too different from what I did in class, and said yes. I quickly discovered the depth and complexity of time series forecasting was so much greater than what I had thought before.\nFor this post, I’ve use python to create several fake datasets to clearly demonstrate time series concepts without the noise and complexity of real-world data. While synthetic, these examples reflect patterns commonly found in actual time series data.\n\n\n\nComplete Series Example\n\n\nThis visualization shows a synthetic time series with trend, seasonality, and random variations. Even in this simplified example, we can observe several key characteristics of time series data:\n\nA clear upward trend over time\nRegular seasonal patterns that repeat at fixed intervals\nRandom variations that create “noise” around these patterns\n\n\n\nTime series forecasting is the process of analyzing historical time-ordered data to predict future values. Unlike other types of predictive modeling, time series forecasting explicitly accounts for the temporal ordering and relationships in the data.\nLife is full of patterns - we find them in nature, buildings, and data. Time series forecasting extracts these patterns from historical data to predict future outcomes. The defining characteristic is the time element: measurements recorded at consistent intervals, whether minutes, hours, days, weeks, or months. Consistency is crucial - if your data sometimes uses daily measurements and other times weekly, you must standardize to a single time unit before analysis.\n\n\n\nBefore you can effectively forecast a time series, you need to understand its components. Most time series can be broken down into four primary components:\n\n\nThe trend represents the long-term progression of your series - whether values are generally increasing, decreasing, or remaining stable over time. As shown in Figure 2, trends can take many forms.\nIdentifying the correct trend pattern is crucial for making accurate long-term forecasts.\n\n\n\nTrend Patterns\n\n\n\n\n\nSeasonality refers to repeating patterns that occur at regular intervals. As shown in Figure 3, these patterns can vary widely:\n\nAnnual patterns (think retail sales peaking during holidays)\nQuarterly patterns (often seen in financial data)\nMonthly, weekly, or daily cycles\nIrregular but predictable peaks (like special events)\nMultiple overlapping seasonal patterns\n\nIn retail data, for example, you might see weekly patterns (higher sales on weekends) overlaid with annual patterns (holiday shopping seasons).\n\n\n\nSeasonality Patterns\n\n\n\n\n\nCyclical patterns are longer-term fluctuations that don’t have a fixed frequency, unlike seasonality. Business cycles that affect performance over several years are a classic example, where boom and bust periods create waves in the data.\n\n\n\nAfter accounting for trend, seasonality, and cyclicity, what remains is the residual or noise component. This represents random, unpredictable variations that can’t be explained by the model. While true randomness can never be predicted, analyzing this component can provide insights into the volatility of your data and the reliability of your forecasts.\n\n\n\n\nTime series decomposition is the process of separating a time series into its component parts. This helps us understand the underlying patterns driving our data.\n\n\n\nStandard Decomposition\n\n\nIn Figure 5, we can see a time series broken down into its components: - The observed data (top panel) - The extracted trend (second panel) - The seasonal pattern (third panel) - The residual noise (bottom panel)\nThis decomposition follows an additive model, represented by the formula:\n\\[Y_t = T_t + S_t + C_t + R_t\\]\nWhere: - \\(Y_t\\) is the observed value at time \\(t\\)  - \\(T_t\\) is the trend component - \\(S_t\\) is the seasonal component - \\(C_t\\) is the cyclical component - \\(R_t\\) is the residual (noise) component\n\n\n\nThere are two main approaches to time series decomposition: additive and multiplicative.\n\n\n\nAdditive vs Multiplicative Series\n\n\n\n\nIn an additive model, the components are added together to form the original series:\n\\[Y_t = T_t + S_t + C_t + R_t\\]\nThis model is appropriate when: - The seasonal variations add a fixed amount regardless of the trend level - For example, ice cream sales might consistently increase by exactly 1,000 units during summer months, whether your baseline is 3,000 or 8,000 units - The data can have negative values (since multiplication by negative values can produce counterintuitive results)\nIn Figure 6 (top panel), you can see that the seasonal variations maintain a consistent amplitude regardless of the trend level.\n\n\n\nIn a multiplicative model, the components are multiplied together:\n\\[Y_t = T_t \\times S_t \\times C_t \\times R_t\\]\nThis model is appropriate when: - The seasonal variations represent a percentage of the trend level - For example, retail sales might increase by 40% during December, meaning a much larger absolute increase when baseline sales are higher - The data must be positive (since multiplication by zero or negative values distorts the pattern)\nIn Figure 6 (bottom panel), note how the seasonal variations grow larger as the trend increases.\n\n\n\nMultiplicative Decomposition\n\n\nFigure 7 shows the decomposition of a multiplicative series. Notice how the seasonal component is expressed as a ratio (values around 1.0) rather than absolute values.\n\n\n\n\nThe MOST important step when approaching time series forecasting is researching the domain your data comes from. If you’re working with sales data from a janitorial products warehouse, take time to understand that industry. Talk to people working in it to discover the key factors driving their sales. This domain knowledge will be invaluable as you build your models and interpret results.\nDomain knowledge helps you:\n\nIdentify expected patterns - Industry experts can tell you about normal seasonal fluctuations\nExplain anomalies - That unexplained spike might be due to a one-time event\nUnderstand structural changes - A sudden shift in the data might be explained by a change in business strategy or market conditions\nSelect appropriate models - Different types of data require different modeling approaches\n\nWithout domain context, you risk misinterpreting the data or making incorrect forecasts. When you combine solid statistical techniques with domain understanding, time series forecasting becomes a powerful tool for business planning, resource usage, and strategic decision-making.\n\n\n\nIn practice, time series data is rarely as clean as our simple decomposition models suggest. Figure 4 highlights several real-world complications:\n\nOutliers: Unusual events can create extreme values (shown in the middle-right panel)\nMissing values: Data collection issues can lead to gaps in your time series\nLevel shifts: Structural changes can cause permanent shifts in the level of your data\nHeteroscedasticity: The variance of your data might change over time, making some periods more volatile than others\n\nAddressing these complications is an important part of time series forecasting, and will be covered in future posts.\n\n\n\nIn this post I introduced the foundational concepts of time series forecasting, including the components that make up a time series and the different approaches to decomposition. Understanding these concepts is crucial before diving into the practical implementation of forecasting models.\nIn my next post, I’ll explore how to implement these concepts in code, working with real-world datasets to build and evaluate forecasting models."
  },
  {
    "objectID": "posts/2025-01-27-machine-learning-forecast/index.html#before-you-start-coding",
    "href": "posts/2025-01-27-machine-learning-forecast/index.html#before-you-start-coding",
    "title": "Time Series Forecasting: Uncovering Patterns in Your Data",
    "section": "",
    "text": "The MOST important step when approaching time series forecasting is researching the domain your data comes from. If you’re working with sales data from a janitorial products warehouse, take time to understand that industry. Talk to people working in it to discover the key factors driving their sales. This domain knowledge will be invaluable as you build your models and interpret results."
  },
  {
    "objectID": "posts/2025-01-27-machine-learning-forecast/index.html#the-basics",
    "href": "posts/2025-01-27-machine-learning-forecast/index.html#the-basics",
    "title": "Time Series Forecasting: Uncovering Patterns in Your Data",
    "section": "",
    "text": "What exactly is time series forecasting? Life is full of patterns - we find them in nature, buildings, and data. Time series forecasting extracts patterns from historical data to predict future outcomes. The defining characteristic is the time element: measurements recorded at consistent intervals, whether minutes, hours, days, weeks, or months. Consistency is crucial - if your data sometimes uses daily measurements and other times weekly, you must standardize to a single time unit before analysis."
  },
  {
    "objectID": "posts/2025-01-27-machine-learning-forecast/index.html#extracting-the-patterns",
    "href": "posts/2025-01-27-machine-learning-forecast/index.html#extracting-the-patterns",
    "title": "Time Series Forecasting: Uncovering Patterns in Your Data",
    "section": "",
    "text": "Once your data is properly structured with consistent time intervals, you can begin identifying patterns and separating the data into its different components through a process called decomposition. Time series data typically contains four main components:\n\n\n\nDaily Energy Consumption Pattern\n\n\nThis visualization reveals one of the most fundamental patterns in energy consumption data: the daily cycle. Notice how energy use drops to its lowest point around 4-5 AM when most people are sleeping, then steadily rises throughout the morning. Consumption peaks in the late afternoon and early evening (around 6-8 PM) before declining again as night falls. This distinctive pattern represents the seasonality component I discussed earlier - a predictable cycle that repeats at regular intervals.\n\n\nThe long-term progression of your series - whether values are generally increasing, decreasing, or remaining stable over time. Trends represent the underlying direction of your data.\n\n\n\nRepeating patterns that occur at regular intervals. These might be daily, weekly, monthly, or yearly cycles depending on your data. Holiday shopping spikes in retail or specific times of year being known for having more storms are examples of seasonality.\n\n\n\nLonger-term patterns that don’t have a fixed frequency. Economic cycles that affect business performance over several years are a classic example, where boom and bust periods create waves in the data.\n\n\n\nThe random variation remaining after accounting for trend, seasonality, and cyclicity. This represents unpredictable fluctuations that can’t be explained by the model.\nThese components combine (either additively or multiplicatively) to create the overall time series pattern.\n\n\n\nTime Series Decomposition of Energy Consumption Data\n\n\nHere we can see a practical application of time series decomposition. The top panel shows the original observed data spanning from 2002 to 2018. Below it, we can see:\n\nTrend Component: The second panel reveals the long-term movement in energy demand, with seasonal fluctuations removed.\nSeasonality Component: The third panel shows regular, predictable patterns that repeat over time.\nResiduals: The bottom panel shows what remains after removing trend and seasonality - the random variation or “noise” that can’t be explained by our model.\n\nThis decomposition is invaluable for understanding the underlying drivers of energy consumption and creating more accurate forecasts."
  },
  {
    "objectID": "posts/2025-01-27-machine-learning-forecast/index.html#understanding-the-patterns",
    "href": "posts/2025-01-27-machine-learning-forecast/index.html#understanding-the-patterns",
    "title": "Time Series Forecasting: Uncovering Patterns in Your Data",
    "section": "",
    "text": "After decomposing your time series, you can use this information to make better predictions. Trends and seasonal patterns often become very noticeable - perhaps sales peak annually during the holiday season or just before school starts.\nThis is why domain knowledge is so critical. Understanding external factors like natural disasters, company sales strategies, or market conditions helps explain outliers, spikes, or dips that might otherwise confuse your model. Without this context, you risk misinterpreting the data or making incorrect forecasts.\nWhen you combine solid statistical techniques with domain understanding, time series forecasting becomes a powerful tool for business planning, resource usage, and strategic decision-making."
  },
  {
    "objectID": "posts/2025-01-27-machine-learning-forecast/index.html#looking-ahead",
    "href": "posts/2025-01-27-machine-learning-forecast/index.html#looking-ahead",
    "title": "Time Series Forecasting: Understanding the Components",
    "section": "",
    "text": "In this post I introduced the foundational concepts of time series forecasting, including the components that make up a time series and the different approaches to decomposition. Understanding these concepts is crucial before diving into the practical implementation of forecasting models.\nIn my next post, I’ll explore how to implement these concepts in code, working with real-world datasets to build and evaluate forecasting models."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Tyler Lowry",
    "section": "About Me",
    "text": "About Me\nAt BYU-Idaho, I serve as both a TA/Tutor in the Mathematics Department and Team Lead at the McKay Maclab, which has strengthened my skills in programming, statistics, and leadership.\nAfter graduating in April 2025, I’ll be continuing my education at Texas A&M in the Statistics Department for a Master’s in Statistical Data Science."
  },
  {
    "objectID": "index.html#featured-projects",
    "href": "index.html#featured-projects",
    "title": "Tyler Lowry",
    "section": "",
    "text": "Check out my Projects page for a showcase of my recent work"
  },
  {
    "objectID": "resume.html#technical-skills",
    "href": "resume.html#technical-skills",
    "title": "Resume",
    "section": "Technical Skills",
    "text": "Technical Skills\n\n\n\nData Science\n\nPython (Pandas, Polars), R (Tidyverse)\nMachine Learning (TensorFlow, Scikit-Learn), Demand Forecasting\nVisualization (Tableau, Power BI, ggplot2)\n\n\n\n\nDevelopment & Engineering\n\nHTML, CSS, JavaScript, TypeScript, PHP, C#\nAPIs, ETL Pipelines, PySpark, SQL\nExcel/VBA, Adobe Suite, Microsoft Office"
  },
  {
    "objectID": "resume.html#tyler-lowry",
    "href": "resume.html#tyler-lowry",
    "title": "Resume",
    "section": "",
    "text": "tjlowry02@gmail.com | LinkedIn"
  },
  {
    "objectID": "posts/2025-01-27-machine-learning-forecast/index.html#what-exactly-is-time-series-forecasting",
    "href": "posts/2025-01-27-machine-learning-forecast/index.html#what-exactly-is-time-series-forecasting",
    "title": "Time Series Forecasting: Understanding the Components",
    "section": "",
    "text": "Time series forecasting is the process of analyzing historical time-ordered data to predict future values. Unlike other types of predictive modeling, time series forecasting explicitly accounts for the temporal ordering and relationships in the data.\nLife is full of patterns - we find them in nature, buildings, and data. Time series forecasting extracts these patterns from historical data to predict future outcomes. The defining characteristic is the time element: measurements recorded at consistent intervals, whether minutes, hours, days, weeks, or months. Consistency is crucial - if your data sometimes uses daily measurements and other times weekly, you must standardize to a single time unit before analysis."
  },
  {
    "objectID": "posts/2025-01-27-machine-learning-forecast/index.html#understanding-time-series-components",
    "href": "posts/2025-01-27-machine-learning-forecast/index.html#understanding-time-series-components",
    "title": "Time Series Forecasting: Understanding the Components",
    "section": "",
    "text": "Before you can effectively forecast a time series, you need to understand its components. Most time series can be broken down into four primary components:\n\n\nThe trend represents the long-term progression of your series - whether values are generally increasing, decreasing, or remaining stable over time. As shown in Figure 2, trends can take many forms.\nIdentifying the correct trend pattern is crucial for making accurate long-term forecasts.\n\n\n\nTrend Patterns\n\n\n\n\n\nSeasonality refers to repeating patterns that occur at regular intervals. As shown in Figure 3, these patterns can vary widely:\n\nAnnual patterns (think retail sales peaking during holidays)\nQuarterly patterns (often seen in financial data)\nMonthly, weekly, or daily cycles\nIrregular but predictable peaks (like special events)\nMultiple overlapping seasonal patterns\n\nIn retail data, for example, you might see weekly patterns (higher sales on weekends) overlaid with annual patterns (holiday shopping seasons).\n\n\n\nSeasonality Patterns\n\n\n\n\n\nCyclical patterns are longer-term fluctuations that don’t have a fixed frequency, unlike seasonality. Business cycles that affect performance over several years are a classic example, where boom and bust periods create waves in the data.\n\n\n\nAfter accounting for trend, seasonality, and cyclicity, what remains is the residual or noise component. This represents random, unpredictable variations that can’t be explained by the model. While true randomness can never be predicted, analyzing this component can provide insights into the volatility of your data and the reliability of your forecasts."
  },
  {
    "objectID": "posts/2025-01-27-machine-learning-forecast/index.html#decomposing-a-time-series",
    "href": "posts/2025-01-27-machine-learning-forecast/index.html#decomposing-a-time-series",
    "title": "Time Series Forecasting: Understanding the Components",
    "section": "",
    "text": "Time series decomposition is the process of separating a time series into its component parts. This helps us understand the underlying patterns driving our data.\n\n\n\nStandard Decomposition\n\n\nIn Figure 5, we can see a time series broken down into its components: - The observed data (top panel) - The extracted trend (second panel) - The seasonal pattern (third panel) - The residual noise (bottom panel)\nThis decomposition follows an additive model, represented by the formula:\n\\[Y_t = T_t + S_t + C_t + R_t\\]\nWhere: - \\(Y_t\\) is the observed value at time \\(t\\)  - \\(T_t\\) is the trend component - \\(S_t\\) is the seasonal component - \\(C_t\\) is the cyclical component - \\(R_t\\) is the residual (noise) component"
  },
  {
    "objectID": "posts/2025-01-27-machine-learning-forecast/index.html#additive-vs.-multiplicative-decomposition",
    "href": "posts/2025-01-27-machine-learning-forecast/index.html#additive-vs.-multiplicative-decomposition",
    "title": "Time Series Forecasting: Understanding the Components",
    "section": "",
    "text": "There are two main approaches to time series decomposition: additive and multiplicative.\n\n\n\nAdditive vs Multiplicative Series\n\n\n\n\nIn an additive model, the components are added together to form the original series:\n\\[Y_t = T_t + S_t + C_t + R_t\\]\nThis model is appropriate when: - The seasonal variations add a fixed amount regardless of the trend level - For example, ice cream sales might consistently increase by exactly 1,000 units during summer months, whether your baseline is 3,000 or 8,000 units - The data can have negative values (since multiplication by negative values can produce counterintuitive results)\nIn Figure 6 (top panel), you can see that the seasonal variations maintain a consistent amplitude regardless of the trend level.\n\n\n\nIn a multiplicative model, the components are multiplied together:\n\\[Y_t = T_t \\times S_t \\times C_t \\times R_t\\]\nThis model is appropriate when: - The seasonal variations represent a percentage of the trend level - For example, retail sales might increase by 40% during December, meaning a much larger absolute increase when baseline sales are higher - The data must be positive (since multiplication by zero or negative values distorts the pattern)\nIn Figure 6 (bottom panel), note how the seasonal variations grow larger as the trend increases.\n\n\n\nMultiplicative Decomposition\n\n\nFigure 7 shows the decomposition of a multiplicative series. Notice how the seasonal component is expressed as a ratio (values around 1.0) rather than absolute values."
  },
  {
    "objectID": "posts/2025-01-27-machine-learning-forecast/index.html#the-critical-role-of-domain-knowledge",
    "href": "posts/2025-01-27-machine-learning-forecast/index.html#the-critical-role-of-domain-knowledge",
    "title": "Time Series Forecasting: Understanding the Components",
    "section": "",
    "text": "The MOST important step when approaching time series forecasting is researching the domain your data comes from. If you’re working with sales data from a janitorial products warehouse, take time to understand that industry. Talk to people working in it to discover the key factors driving their sales. This domain knowledge will be invaluable as you build your models and interpret results.\nDomain knowledge helps you:\n\nIdentify expected patterns - Industry experts can tell you about normal seasonal fluctuations\nExplain anomalies - That unexplained spike might be due to a one-time event\nUnderstand structural changes - A sudden shift in the data might be explained by a change in business strategy or market conditions\nSelect appropriate models - Different types of data require different modeling approaches\n\nWithout domain context, you risk misinterpreting the data or making incorrect forecasts. When you combine solid statistical techniques with domain understanding, time series forecasting becomes a powerful tool for business planning, resource usage, and strategic decision-making."
  },
  {
    "objectID": "posts/2025-01-27-machine-learning-forecast/index.html#real-world-complications",
    "href": "posts/2025-01-27-machine-learning-forecast/index.html#real-world-complications",
    "title": "Time Series Forecasting: Understanding the Components",
    "section": "",
    "text": "In practice, time series data is rarely as clean as our simple decomposition models suggest. Figure 4 highlights several real-world complications:\n\nOutliers: Unusual events can create extreme values (shown in the middle-right panel)\nMissing values: Data collection issues can lead to gaps in your time series\nLevel shifts: Structural changes can cause permanent shifts in the level of your data\nHeteroscedasticity: The variance of your data might change over time, making some periods more volatile than others\n\nAddressing these complications is an important part of time series forecasting, and will be covered in future posts."
  },
  {
    "objectID": "posts/2025-02-21-ts-forecast/index.html#stationarity-data",
    "href": "posts/2025-02-21-ts-forecast/index.html#stationarity-data",
    "title": "Time Series Forecasting: ARIMA and ETS Models",
    "section": "",
    "text": "A stationary time series has consistent statistical properties over time - constant mean, variance, and autocorrelation structure. Most forecasting models require stationarity to work effectively.\nSigns of non-stationarity include: - Visible trend (upward/downward) - Changing variance over time - Seasonal patterns"
  },
  {
    "objectID": "posts/2025-02-21-ts-forecast/index.html#differencing",
    "href": "posts/2025-02-21-ts-forecast/index.html#differencing",
    "title": "Time Series Forecasting: ARIMA and ETS Models",
    "section": "",
    "text": "Differencing transforms non-stationary data into stationary data by subtracting consecutive observations: - First-order differencing: y’ₜ = yₜ - yₜ₋₁ - Second-order differencing: y’’ₜ = y’ₜ - y’ₜ₋₁\nDifferent levels of differencing are used depending on how persistent trends are in your data, but the goal is to flatten out the trend in the data."
  },
  {
    "objectID": "posts/2025-02-21-ts-forecast/index.html#arima-models",
    "href": "posts/2025-02-21-ts-forecast/index.html#arima-models",
    "title": "Time Series Forecasting: ARIMA and ETS Models",
    "section": "",
    "text": "ARIMA(p,d,q) combines three components: - p (AutoRegressive): Number of lag observations used - d (Integrated): Number of differencing required to make stationary - q (Moving Average): Size of moving average window\nFor example, ARIMA(1,1,1) uses 1 lag value, applies first differencing, and includes 1 moving average term.\nSeasonal patterns can be handled with SARIMA, adding seasonal parameters (P,D,Q,m) where m is the seasonality period."
  },
  {
    "objectID": "posts/2025-02-21-ts-forecast/index.html#ets-models",
    "href": "posts/2025-02-21-ts-forecast/index.html#ets-models",
    "title": "Time Series Forecasting: ARIMA and ETS Models",
    "section": "",
    "text": "ETS stands for Error, Trend, Seasonality, focusing on the components we decompose time series into: - Error: Additive (A) or Multiplicative (M) - Trend: None (N), Additive (A), or Multiplicative (M), with optional dampening - Seasonality: None (N), Additive (A), or Multiplicative (M)\nFor example, ETS(A,A,N) handles data with additive error, additive trend, and no seasonality."
  },
  {
    "objectID": "posts/2025-02-21-ts-forecast/index.html#auto-arima",
    "href": "posts/2025-02-21-ts-forecast/index.html#auto-arima",
    "title": "Time Series Forecasting: ARIMA and ETS Models",
    "section": "",
    "text": "Auto ARIMA algorithms automatically search for optimal ARIMA parameters by: 1. Testing different combinations of p, d, q values 2. Using information criteria (AIC, BIC) to select the best model 3. Testing for stationarity and determining proper differencing\nfrom pmdarima import auto_arima\n\nmodel = auto_arima(data, seasonal=True, m=12,\n                  start_p=0, max_p=3, start_q=0, max_q=3,\n                  d=None, max_d=2, trace=True)\n\nseasonal=True: Tells the function to look for seasonal patterns in the data\nm=12: Sets the seasonal period to 12 (good for monthly data)\nstart_p=0, max_p=3: Tests AR terms from 0 to 3\nstart_q=0, max_q=3: Tests MA terms from 0 to 3\nd=None, max_d=2: Lets the function automatically determine differencing up to order 2\ntrace=True: Prints the model results as they’re being tested\n\nAutoArima will go through possible combinations of these parameters in a step-wise method finding the combination that best minimizes AIC and BIC"
  },
  {
    "objectID": "posts/2025-02-21-ts-forecast/index.html#when-to-use-each-model",
    "href": "posts/2025-02-21-ts-forecast/index.html#when-to-use-each-model",
    "title": "Time Series Forecasting: ARIMA and ETS Models",
    "section": "",
    "text": "Use ARIMA when: - Data shows complex autocorrelation patterns - You want to model time series based on previous values and errors, use the past to explain the present - Data becomes stationary after differencing\nUse ETS when: - You have clear trend and seasonal patterns - You have limited historical data - Want the trend modeled into the data\nFor adding a resources section at the end of your blog post, here’s a simple and effective approach:"
  },
  {
    "objectID": "posts/2025-02-21-ts-forecast/index.html#resources",
    "href": "posts/2025-02-21-ts-forecast/index.html#resources",
    "title": "Time Series Forecasting: ARIMA and ETS Models",
    "section": "",
    "text": "Online Resources:\n\nAutoArima Documentation\nTime series textbook\nStatsmodel ETS Documentation"
  },
  {
    "objectID": "posts/2025-03-17-ensembling-models/index.html",
    "href": "posts/2025-03-17-ensembling-models/index.html",
    "title": "Boosting Forecast Accuracy Through Model Ensembling",
    "section": "",
    "text": "One of the most helpful concepts I have found while learning about time series forecasting is ‘ensembling’ models. This is the process of combining the predictions of two models and averaging them out somehow.\n\n\nDifferent forecasting models capture different aspects of time series data. Statistical models like ARIMA excel at capturing autocorrelation patterns, while machine learning models like LightGBM often better capture complex non-linear relationships. By combining these complementary strengths, we can get the best of both models.\n\n\n\nEnsembled Model\n\n\n\n\n\nThe most straightforward ensembling approach, and the approach I have been using is weighted averaging. Below is a function I wrote to ensemble the predicitons to a LightGBM and ARIMA model:\ndef ensemble_predictions(lgb_predictions, arima_predictions, alpha):\n    \"\"\"\n    Combine predictions from LightGBM and ARIMA models using a weighted average.\n\n    Args:\n    lgb_predictions (np.array): Array of predictions from the LightGBM model.\n    arima_predictions (np.array): Array of predictions from the ARIMA model.\n    alpha (float): Weighting factor for the LightGBM predictions in the ensemble.\n\n    Returns:\n    np.array: Array of ensemble predictions, combining both models' outputs.\n    \"\"\"\n\n    ensemble_pred = (\n        alpha * lgb_predictions + (1 - alpha) * arima_predictions\n    )\n    ensemble_pred = np.maximum(ensemble_pred, 0).round()\n    return ensemble_pred\nThis function: 1. Takes predictions from both models 2. Applies a weighting factor alpha to control each model’s influence 3. Ensures predictions are non-negative and rounded to integers (important for count data)\nThe ‘alpha’ value determines how much weight I want on each. I tested many different and found a weight of 35% arima, 65% LightGBM gave the best results because the LightGBM model tended to overpredict large spikes and dips in the data, and ARIMA was able to smooth it out limiting the impact of the extereme lows and highs.\n\n\n\nThis function is specific to the code I have where I had a set of LightGBM predictions and a set of ARIMA predictions, but it can easily be changed to fit other cicrumstances.\ndef ensemble_predictions(model_predictions, weights=None):\n    \"\"\"\n    Combine predictions from multiple models using a weighted average.\n\n    Args:\n    model_predictions (list): List of numpy arrays, each containing predictions from different models.\n    weights (list, optional): List of weighting factors for each model. \n                             If None, equal weights are assigned.\n\n    Returns:\n    np.array: Array of ensemble predictions, combining all models' outputs.\n    \"\"\"\n    import numpy as np\n    \n    n_models = len(model_predictions)\n    \n    # If weights given, use equal weights\n    if weights is None:\n        weights = [1.0 / n_models] * n_models\n    \n    ensemble_pred = np.zeros_like(model_predictions[0])\n    for i, predictions in enumerate(model_predictions):\n        ensemble_pred += weights[i] * predictions\n    \n    ensemble_pred = np.maximum(ensemble_pred, 0).round()\n    \n    return ensemble_pred\nAn example of how this could be used is:\nmodel1_preds = np.array([100, 105, 110, 115, 120])\nmodel2_preds = np.array([90, 100, 105, 110, 125])\nmodel3_preds = np.array([105, 110, 115, 120, 130])\nmodel4_preds = np.array([95, 105, 108, 112, 118])\n\n# Assign weights\nweights = [0.4, 0.1, 0.3, 0.2]\nensemble = ensemble_predictions(\n    [model1_preds, model2_preds, model3_preds, model4_preds], \n    weights=weights\n)\nThis would take all 4 models and 40% weight to model 1, 10% to model 2, 30% to model 3, and 20% to model 4. By combining models together it can help reduce the impact of exterme predictions."
  },
  {
    "objectID": "posts/2025-03-17-ensembling-models/index.html#why-ensemble-models",
    "href": "posts/2025-03-17-ensembling-models/index.html#why-ensemble-models",
    "title": "Boosting Forecast Accuracy Through Model Ensembling",
    "section": "",
    "text": "Different forecasting models capture different aspects of time series data. Statistical models like ARIMA excel at capturing autocorrelation patterns, while machine learning models like LightGBM often better capture complex non-linear relationships. By combining these complementary strengths, we can get the best of both models.\n\n\n\nEnsembled Model"
  },
  {
    "objectID": "posts/2025-03-17-ensembling-models/index.html#simple-weighted-averaging",
    "href": "posts/2025-03-17-ensembling-models/index.html#simple-weighted-averaging",
    "title": "Boosting Forecast Accuracy Through Model Ensembling",
    "section": "",
    "text": "The most straightforward ensembling approach, and the approach I have been using is weighted averaging. Below is a function I wrote to ensemble the predicitons to a LightGBM and ARIMA model:\ndef ensemble_predictions(lgb_predictions, arima_predictions, alpha):\n    \"\"\"\n    Combine predictions from LightGBM and ARIMA models using a weighted average.\n\n    Args:\n    lgb_predictions (np.array): Array of predictions from the LightGBM model.\n    arima_predictions (np.array): Array of predictions from the ARIMA model.\n    alpha (float): Weighting factor for the LightGBM predictions in the ensemble.\n\n    Returns:\n    np.array: Array of ensemble predictions, combining both models' outputs.\n    \"\"\"\n\n    ensemble_pred = (\n        alpha * lgb_predictions + (1 - alpha) * arima_predictions\n    )\n    ensemble_pred = np.maximum(ensemble_pred, 0).round()\n    return ensemble_pred\nThis function: 1. Takes predictions from both models 2. Applies a weighting factor alpha to control each model’s influence 3. Ensures predictions are non-negative and rounded to integers (important for count data)\nThe ‘alpha’ value determines how much weight I want on each. I tested many different and found a weight of 35% arima, 65% LightGBM gave the best results because the LightGBM model tended to overpredict large spikes and dips in the data, and ARIMA was able to smooth it out limiting the impact of the extereme lows and highs."
  },
  {
    "objectID": "posts/2025-03-17-ensembling-models/index.html#conclusion",
    "href": "posts/2025-03-17-ensembling-models/index.html#conclusion",
    "title": "Boosting Forecast Accuracy Through Model Ensembling",
    "section": "",
    "text": "This function is specific to the code I have where I had a set of LightGBM predictions and a set of ARIMA predictions, but it can easily be changed to fit other cicrumstances.\ndef ensemble_predictions(model_predictions, weights=None):\n    \"\"\"\n    Combine predictions from multiple models using a weighted average.\n\n    Args:\n    model_predictions (list): List of numpy arrays, each containing predictions from different models.\n    weights (list, optional): List of weighting factors for each model. \n                             If None, equal weights are assigned.\n\n    Returns:\n    np.array: Array of ensemble predictions, combining all models' outputs.\n    \"\"\"\n    import numpy as np\n    \n    n_models = len(model_predictions)\n    \n    # If weights given, use equal weights\n    if weights is None:\n        weights = [1.0 / n_models] * n_models\n    \n    ensemble_pred = np.zeros_like(model_predictions[0])\n    for i, predictions in enumerate(model_predictions):\n        ensemble_pred += weights[i] * predictions\n    \n    ensemble_pred = np.maximum(ensemble_pred, 0).round()\n    \n    return ensemble_pred\nAn example of how this could be used is:\nmodel1_preds = np.array([100, 105, 110, 115, 120])\nmodel2_preds = np.array([90, 100, 105, 110, 125])\nmodel3_preds = np.array([105, 110, 115, 120, 130])\nmodel4_preds = np.array([95, 105, 108, 112, 118])\n\n# Assign weights\nweights = [0.4, 0.1, 0.3, 0.2]\nensemble = ensemble_predictions(\n    [model1_preds, model2_preds, model3_preds, model4_preds], \n    weights=weights\n)\nThis would take all 4 models and 40% weight to model 1, 10% to model 2, 30% to model 3, and 20% to model 4. By combining models together it can help reduce the impact of exterme predictions."
  },
  {
    "objectID": "posts/2025-4-1-streamlit-functions/index.html",
    "href": "posts/2025-4-1-streamlit-functions/index.html",
    "title": "Organizing Streamlit Dashboards with Functions",
    "section": "",
    "text": "When I first started building my dashboard with Streamlit, it was working out pretty good, however after a while my ‘dashboard.py’ script had turned into a nightmare to edit. It was over 1000 lines of code and trying to find the right parts of the script to edit was becoming a nightmare. I had been coding, but not organizing or structuring it at all.\n\n\n\nDashboard Homepage\n\n\n\n\nI decided I needed to take a step back and think about a better way to do it. I realized one step I could take was have a script where all my functions were defined, then import them into my dashboard, something like this:\nfrom forecast_script import (\n    train_test_split,\n    create_features,\n    run_naive_forecast,\n    run_sarima,\n    run_ets,\n    run_xgboost,\n    evaluate_forecasts\n)\nThis was awesome as all I had to do in actual dashboard was call the function and it made it easier to manage because I knew streamlit code was in the dashboard script and my functions were in the forecast script. But eventually this got too complicated too so I took it a step further and made each page in my dashboard a file and function. now I had two more functions and two more python files.\nfrom forecast_script import (\n    train_test_split,\n    create_features,\n    run_naive_forecast,\n    run_sarima,\n    run_ets,\n    run_xgboost,\n    evaluate_forecasts\n)\n\ndef model_comparison_page():\n    #....\nfrom retails_script import (\n    load_and_preprocess,\n    detect_seasonality_periods\n)\nfrom resampling_functions import(\n    add_resampling_section\n)\n\ndef data_overview_page():\n    #...\nthat meant my dashboard.py file went from over 1000 lines of code (and would’ve been much more in the end) and a nightmare to edit to a organized file structure with different files for different things and only about 20 lines of code.\nimport streamlit as st\nfrom data_overview_page import data_overview_page\nfrom model_comparison import model_comparison_page\n\n# Set up the page config first\nst.set_page_config(\n    page_title=\"Time Series Analysis Dashboard\",\n    layout=\"wide\"\n)\n\nselected_page = st.radio(\n    \"Select Dashboard Section:\",\n    [\"Data Overview\", \"Model Comparison\"],\n    horizontal=True\n)\n\nif selected_page == \"Data Overview\":\n    data_overview_page()\nelif selected_page == \"Model Comparison\":\n    model_comparison_page()\n\n\n\nI know it may seem like I didn’t really do anything and I just ‘shoved it under the bed’ so things look cleaner. But working on everything became so much easier. Refactoring my Streamlit dashboard using a function-based approach dramatically improved its maintainability. What was once a sprawling script is now a well-organized collection of focused functions, each with a clear purpose. I realized I had basically just applied abstraction, a concept I learned about in my programming with classes course, to Python.\nThis modular approach is actually a fundamental programming principle that extends beyond just Streamlit applications. When functions are organized logically and serve a single purpose, it makes the entire development process smoother and more enjoyable. I know that all may seem like common sense to software engineers, but as a data sciencetist its something I realize I need to think about much more because I can’t just make a new jupyter notebook called ‘project_V4.ipynb’ everytime my codes gets messy."
  },
  {
    "objectID": "posts/2025-4-1-streamlit-functions/index.html#the-function-based-solution",
    "href": "posts/2025-4-1-streamlit-functions/index.html#the-function-based-solution",
    "title": "Organizing Streamlit Dashboards with Functions",
    "section": "",
    "text": "I decided I needed to take a step back and think about a better way to do it. I realized one step I could take was have a script where all my functions were defined, then import them into my dashboard, something like this:\nfrom forecast_script import (\n    train_test_split,\n    create_features,\n    run_naive_forecast,\n    run_sarima,\n    run_ets,\n    run_xgboost,\n    evaluate_forecasts\n)\nThis was awesome as all I had to do in actual dashboard was call the function and it made it easier to manage because I knew streamlit code was in the dashboard script and my functions were in the forecast script. But eventually this got too complicated too so I took it a step further and made each page in my dashboard a file and function. now I had two more functions and two more python files.\nfrom forecast_script import (\n    train_test_split,\n    create_features,\n    run_naive_forecast,\n    run_sarima,\n    run_ets,\n    run_xgboost,\n    evaluate_forecasts\n)\n\ndef model_comparison_page():\n    #....\nfrom retails_script import (\n    load_and_preprocess,\n    detect_seasonality_periods\n)\nfrom resampling_functions import(\n    add_resampling_section\n)\n\ndef data_overview_page():\n    #...\nthat meant my dashboard.py file went from over 1000 lines of code (and would’ve been much more in the end) and a nightmare to edit to a organized file structure with different files for different things and only about 20 lines of code.\nimport streamlit as st\nfrom data_overview_page import data_overview_page\nfrom model_comparison import model_comparison_page\n\n# Set up the page config first\nst.set_page_config(\n    page_title=\"Time Series Analysis Dashboard\",\n    layout=\"wide\"\n)\n\nselected_page = st.radio(\n    \"Select Dashboard Section:\",\n    [\"Data Overview\", \"Model Comparison\"],\n    horizontal=True\n)\n\nif selected_page == \"Data Overview\":\n    data_overview_page()\nelif selected_page == \"Model Comparison\":\n    model_comparison_page()"
  },
  {
    "objectID": "posts/2025-4-1-streamlit-functions/index.html#conclusion",
    "href": "posts/2025-4-1-streamlit-functions/index.html#conclusion",
    "title": "Organizing Streamlit Dashboards with Functions",
    "section": "",
    "text": "I know it may seem like I didn’t really do anything and I just ‘shoved it under the bed’ so things look cleaner. But working on everything became so much easier. Refactoring my Streamlit dashboard using a function-based approach dramatically improved its maintainability. What was once a sprawling script is now a well-organized collection of focused functions, each with a clear purpose. I realized I had basically just applied abstraction, a concept I learned about in my programming with classes course, to Python.\nThis modular approach is actually a fundamental programming principle that extends beyond just Streamlit applications. When functions are organized logically and serve a single purpose, it makes the entire development process smoother and more enjoyable. I know that all may seem like common sense to software engineers, but as a data sciencetist its something I realize I need to think about much more because I can’t just make a new jupyter notebook called ‘project_V4.ipynb’ everytime my codes gets messy."
  }
]