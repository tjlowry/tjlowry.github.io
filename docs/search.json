[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Data Science Blog",
    "section": "",
    "text": "Time Series Forecasting: Understanding the Components\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime Series Forecasting: First Models in R and Python\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tyler Lowry",
    "section": "",
    "text": "I’m a Data Science student at BYU-Idaho passionate about turning complex data into actionable insights. With a minor in Accounting and a Machine Learning Certificate, I hope to bridge the gap between business and statistics.\nAt BYU-Idaho, I serve as both a TA/Tutor in the Mathematics Department and Team Lead at the McKay Maclab, which has strengthened my skills in programming, statistics, and leadership.\nAfter graduating in April 2025, I’ll be continuing my education at Texas A&M for a Master’s in Statistical Data Science.\n\n\n\n\n\n\n\n\n\n\n\nPython (Pandas, Polars, Scikit-learn, TensorFlow)\nR\nSQL\n\n\n\n\n\nTableau\nPower BI\nggplot\nPlotly, lets_plot, Seaborn\n\n\n\n\n\n\nSQL Database Optimization\nAWS EC2/S3\nDatabase Design & Architecture\n\n\n\n\n\nPHP (Filament), C#\nJavaScript/TypeScript\nHTML/CSS\n\n\n\n\n\n\n\n\nCheck out my Projects page for a showcase of my recent work"
  },
  {
    "objectID": "index.html#skills",
    "href": "index.html#skills",
    "title": "Tyler Lowry",
    "section": "",
    "text": "Python (Pandas, Polars, Scikit-learn, TensorFlow)\nR\nSQL\n\n\n\n\n\nTableau\nPower BI\nggplot\nPlotly, lets_plot, Seaborn\n\n\n\n\n\n\nSQL Database Optimization\nAWS EC2/S3\nDatabase Design & Architecture\n\n\n\n\n\nPHP (Filament), C#\nJavaScript/TypeScript\nHTML/CSS"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "tjlowry02@gmail.com | LinkedIn"
  },
  {
    "objectID": "resume.html#education",
    "href": "resume.html#education",
    "title": "Resume",
    "section": "Education",
    "text": "Education\nBrigham Young University-Idaho | Rexburg, ID\nBS, Data Science - Accounting Minor - Machine Learning Certificate\nExpected Graduation: April 2025\nGPA: 3.89"
  },
  {
    "objectID": "resume.html#skills",
    "href": "resume.html#skills",
    "title": "Resume",
    "section": "",
    "text": "R, Python, Pandas, Polars, SQL\nMachine Learning, Demand Forecasting, TensorFlow, Scikit-Learn\nTableau, Power BI\n\n\n\n\n\nHTML, CSS, JavaScript, PHP, PySpark\nAPIs, Data Pipelines\nExcel, VBA, Adobe Suite"
  },
  {
    "objectID": "resume.html#professional-experience",
    "href": "resume.html#professional-experience",
    "title": "Resume",
    "section": "Professional Experience",
    "text": "Professional Experience\n\nMachine Learning Engineer Intern | PIC Business Systems\nMay 2023 - Sept 2023 | San Antonio, TX\n\nDeveloped a 90-day sales forecasting ensembled model and an interactive dashboard for product performance, resulting in a 30% improvement in median forecast MAPE\nEnhanced data quality by integrating external factors such as weather and date, using APIs and Pandas to create additional features for the model\nAutomated monthly forecasting and database updates by leveraging AWS Lambda and S3 Buckets, enabling easy access to forecasts and reducing manual efforts\n\n\n\nTA/Tutor | BYU-Idaho Department of Mathematics\nApril 2024 - Present | Rexburg, ID\n\nServed as a teaching assistant and tutor for “Intro to Programming,” “Math for the Real World,” “Data Wrangling and Visualization,” “Data Science Programming,” and “Biostatistics”\nLed group and one-on-one tutoring sessions focused on Python, R, Excel, and statistics\nProvided general data science support at the Data Science Tutoring Lab, assisting students across various data science and statistics courses with Python, R, and other coursework-related questions\n\n\n\nTeam Lead/Developer | BYU-Idaho McKay Maclab\nJan 2023 - Present | Rexburg, ID\n\nCollaborated with a team of 3 in full stack development of Maclab website and 3D printing queue\nConducted interviews, reviewed resumes to screen candidates, and provided training to new employees\nFacilitated weekly meetings to discuss goals, track task progress, and deliver short training sessions to the team\nDesigned interactive streamlit dashboards to track KPI performance, resulting in a 14% average improvement across all metrics\n\n\n\nIntern | George W. Lowry Inc.\nSept 2023 - Dec 2023 | Salida, CA\n\nMigrated legacy database system to AWS and optimized database architecture by eliminating redundant joins, improving foreign key usage, and refining indexes, resulting in a nearly two-hour reduction in report generation runtime\nIdentified and implemented an AWS EC2 instance, increasing computing power while lowering costs, reducing server crashes, and improving efficiency\nDeveloped a CRM using a PHP framework, enhancing location data tracking and streamlining communication between sales team and delivery drivers for improved logistics"
  },
  {
    "objectID": "posts/2024-01-16-machine-learning-forecast/index.html",
    "href": "posts/2024-01-16-machine-learning-forecast/index.html",
    "title": "Building a Sales Forecasting Model",
    "section": "",
    "text": "During my internship at PIC Business Systems, I had the opportunity to develop a sales forecasting model that improved prediction accuracy by 30%. Here’s how I approached it."
  },
  {
    "objectID": "posts/2024-01-16-machine-learning-forecast/index.html#the-challenge",
    "href": "posts/2024-01-16-machine-learning-forecast/index.html#the-challenge",
    "title": "Building a Sales Forecasting Model",
    "section": "The Challenge",
    "text": "The Challenge\nAccurately predicting sales for the next 90 days was crucial for inventory management and business planning. The existing system relied heavily on manual predictions and didn’t account for external factors."
  },
  {
    "objectID": "posts/2024-01-16-machine-learning-forecast/index.html#the-solution",
    "href": "posts/2024-01-16-machine-learning-forecast/index.html#the-solution",
    "title": "Building a Sales Forecasting Model",
    "section": "The Solution",
    "text": "The Solution\nI developed an ensemble model that combined: - XGBoost - Prophet - SARIMA\nThe model incorporated external factors like: - Weather patterns - Seasonal trends - Historical sales data - Market indicators\n[Continue reading for more technical details…]"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Projects",
    "section": "",
    "text": "My Projects\nThis page will be updated soon with projects I have worked on."
  },
  {
    "objectID": "about.html#what-im-passionate-about",
    "href": "about.html#what-im-passionate-about",
    "title": "About Me",
    "section": "",
    "text": "As a Machine Learning Engineer and Developer, I love tackling challenges that combine data science with practical applications. Whether it’s developing sales forecasting models or optimizing database performance, I enjoy seeing how data can drive real business impact."
  },
  {
    "objectID": "about.html#current-role",
    "href": "about.html#current-role",
    "title": "About Me",
    "section": "",
    "text": "At BYU-Idaho, I serve as both a TA/Tutor in the Mathematics Department and Team Lead at the McKay Maclab. I get to combine my technical skills with leadership, whether I’m helping students grasp programming concepts or leading development projects."
  },
  {
    "objectID": "about.html#beyond-the-code",
    "href": "about.html#beyond-the-code",
    "title": "About Me",
    "section": "",
    "text": "When I am not working with data, I love to spend time with my wife and our adopted cat George, a stray we found as a kitten in a California warehouse. I am a huge sports fan and root for all the Arizona-based sports teams even though I’m from San Antonio, TX. I became a fan through my mom, who grew up in Arizona. Some other hobbies of mine are 3d printing, history, and sci-fi."
  },
  {
    "objectID": "about.html#connect-me",
    "href": "about.html#connect-me",
    "title": "About Me",
    "section": "",
    "text": "Connect with me on LinkedIn!"
  },
  {
    "objectID": "posts/2025-02-21-ts-forecast/index.html",
    "href": "posts/2025-02-21-ts-forecast/index.html",
    "title": "Time Series Forecasting: First Models in R and Python",
    "section": "",
    "text": "In my previous post, I introduced the fundamental concepts of time series forecasting - the importance of domain knowledge, consistent time intervals, and the key components that make up time series data (trend, seasonality, cyclicity, and residuals). Now, let’s take the next step and implement some basic forecasting models in both R and Python.\n\n\nWhile Python has become my go-to language for data science work, I feel R is more intutive for certain aspects of time series forecasting due to the great packages which have been devolped. Each has distinct advantages:\n\nR excels with its purpose-built statistical packages and elegant handling of time series objects\nPython offers incredible flexibility, integration with other systems, and powerful machine learning capabilities\n\nLearning both approaches gives you a more complete toolkit and the ability to choose the right tool for each specific forecasting challenge.\n\n\n\nBefore diving into models, we need data. For this example, let’s use a classic time series dataset: monthly airline passenger numbers. This dataset shows a clear upward trend and strong seasonality, making it perfect for demonstrating basic forecasting techniques.\n\n\n\nR has specialized libraries designed specifically for time series analysis. The forecast package, created by Rob Hyndman, is particularly powerful and user-friendly.\n\n\n# Install and load necessary packages\ninstall.packages(c(\"forecast\", \"tseries\"))\nlibrary(forecast)\nlibrary(tseries)\nlibrary(ggplot2)\n\n# Load the airlines dataset (included in R)\ndata(\"AirPassengers\")\nair &lt;- AirPassengers  # Monthly totals of airline passengers from 1949-1960\n\n# Examine the data\nclass(air)  # Should be \"ts\" (time series)\nfrequency(air)  # 12 = monthly data\nstart(air)  # Starting time period\nend(air)  # Ending time period\n\n# Plot the data\nautoplot(air) + \n  ggtitle(\"Monthly Airline Passenger Numbers 1949-1960\") + \n  xlab(\"Year\") + \n  ylab(\"Passengers (thousands)\")\n\n# Decompose the time series\nair_decomp &lt;- decompose(air, type = \"multiplicative\")\nautoplot(air_decomp)\n\n# Check stationarity with Augmented Dickey-Fuller test\nadf.test(air)  # p-value &gt; 0.05 indicates non-stationarity\n\n# Differencing to achieve stationarity\nair_diff &lt;- diff(log(air))\nadf.test(air_diff)  # Should now be stationary\n\n# Examine ACF and PACF for model parameter selection\nacf(air_diff, main=\"ACF of Differenced Log Passenger Data\")\npacf(air_diff, main=\"PACF of Differenced Log Passenger Data\")\n\n# Fit an ARIMA model automatically\nair_model &lt;- auto.arima(air, seasonal = TRUE)\nsummary(air_model)\n\n# Forecast the next 24 months\nair_forecast &lt;- forecast(air_model, h = 24)\n\n# Plot the forecast\nautoplot(air_forecast) + \n  ggtitle(\"24-Month Forecast of Airline Passengers\")\nThis R example walks through the complete process: 1. Loading and examining the data 2. Visualizing and decomposing the time series 3. Testing for stationarity (crucial for ARIMA models) 4. Using auto.arima() to automatically select the best parameters 5. Generating and visualizing a forecast\nThe auto.arima() function automatically determines the appropriate ARIMA model parameters (p, d, q) and their seasonal counterparts (P, D, Q), saving us from manual parameter tuning.\n\n\n\n\nPython offers multiple libraries for time series forecasting. The statsmodels package provides statistical models while pandas handles the data manipulation.\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom pandas.plotting import register_matplotlib_converters\nregister_matplotlib_converters()\n\n# Load the AirPassengers dataset\nair = pd.read_csv('https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv',\n                  index_col='Month', parse_dates=True)\nair.columns = ['Passengers']\n\n# Plot the data\nplt.figure(figsize=(10, 6))\nplt.plot(air)\nplt.title('Monthly Airline Passenger Numbers 1949-1960')\nplt.xlabel('Year')\nplt.ylabel('Passengers (thousands)')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# Decompose the time series\ndecomposition = seasonal_decompose(air, model='multiplicative')\nfig = decomposition.plot()\nfig.set_size_inches(10, 8)\nplt.tight_layout()\nplt.show()\n\n# Check stationarity with Augmented Dickey-Fuller test\ndef adf_test(series):\n    result = adfuller(series.dropna())\n    print('ADF Statistic: %f' % result[0])\n    print('p-value: %f' % result[1])\n    print('Critical Values:')\n    for key, value in result[4].items():\n        print('\\t%s: %.3f' % (key, value))\n    if result[1] &lt;= 0.05:\n        print(\"Conclusion: The series is stationary\")\n    else:\n        print(\"Conclusion: The series is non-stationary\")\n\nadf_test(air['Passengers'])\n\n# Take log and difference to make stationary\nair['LogPassengers'] = np.log(air['Passengers'])\nair['LogDiffPassengers'] = air['LogPassengers'].diff()\nadf_test(air['LogDiffPassengers'].dropna())\n\n# Plot ACF and PACF for parameter selection\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\nplot_acf(air['LogDiffPassengers'].dropna(), ax=ax1)\nplot_pacf(air['LogDiffPassengers'].dropna(), ax=ax2)\nplt.tight_layout()\nplt.show()\n\n# Fit a SARIMA model (with parameters based on ACF/PACF)\n# For this dataset, a SARIMA(1,1,1)(1,1,1,12) often works well\nmodel = SARIMAX(air['Passengers'], order=(1, 1, 1), seasonal_order=(1, 1, 1, 12))\nresults = model.fit()\nprint(results.summary())\n\n# Forecast the next 24 months\nforecast_steps = 24\nforecast_index = pd.date_range(start=air.index[-1], periods=forecast_steps+1, freq='MS')[1:]\nforecast = results.forecast(forecast_steps)\nforecast_df = pd.DataFrame(forecast, index=forecast_index, columns=['Forecast'])\n\n# Plot the actual data and the forecast\nplt.figure(figsize=(12, 6))\nplt.plot(air['Passengers'], label='Historical data')\nplt.plot(forecast_df, label='Forecast')\nplt.title('24-Month Forecast of Airline Passengers')\nplt.xlabel('Year')\nplt.ylabel('Passengers (thousands)')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# Calculate and plot confidence intervals\npred = results.get_prediction(start=air.index[-1], end=forecast_index[-1])\npred_conf = pred.conf_int()\n\nplt.figure(figsize=(12, 6))\nplt.plot(air['Passengers'], label='Historical data')\nplt.plot(pred.predicted_mean, label='Forecast')\nplt.fill_between(pred_conf.index, \n                 pred_conf.iloc[:, 0], \n                 pred_conf.iloc[:, 1], \n                 color='k', alpha=0.2)\nplt.title('24-Month Forecast with 95% Confidence Interval')\nplt.xlabel('Year')\nplt.ylabel('Passengers (thousands)')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\nThe Python example follows a similar workflow to the R version, but with Python-specific libraries:\n\nLoading data with pandas\nVisualizing with matplotlib\nDecomposing and testing stationarity\nFitting a SARIMA model using statsmodels\nForecasting future values and plotting with confidence intervals\n\n\n\n\n\nAs you can see, while the overall process is similar, there are some notable differences:\n\nData Handling: R has native support for time series objects, while Python relies on pandas DateTime indexes\nModel Selection: R’s auto.arima() automatically selects parameters, while in Python we typically need to analyze ACF/PACF plots or use grid search\nVisualization: R’s plotting is more concise with the forecast package, while Python offers more customization with matplotlib\nPerformance: For larger datasets, Python may offer better performance, especially when integrated with other machine learning workflows\n\n\n\n\nWhile ARIMA and SARIMA models are great starting points, there are many other approaches to explore:\n\nExponential Smoothing: Simple yet powerful, especially for data with strong seasonality\nProphet: Facebook’s forecasting tool that handles holidays and events effectively\nLSTM Networks: Deep learning approaches for complex sequential patterns\nXGBoost with Lag Features: Machine learning with engineered time features\n\nI’ll cover these more advanced techniques in future posts, showing implementations in both R and Python where appropriate.\n\n\n\nThese examples demonstrate basic forecasting models in R and Python, but they’re just the beginning. The choice between R and Python often comes down to your specific needs and existing workflow:\n\nChoose R if your work is primarily statistical and you value specialized time series tools\nChoose Python if you need integration with larger systems or want to leverage deep learning approaches\n\nIn my experience, learning both gives you the most flexibility. Many data scientists start with R for its statistical focus, then migrate to Python as they integrate forecasting into broader data pipelines and applications.\nIn the next post, I’ll dive deeper into model evaluation and selection - how to measure forecast accuracy and choose the right model for your specific time series problem."
  },
  {
    "objectID": "posts/2025-02-21-ts-forecast/index.html#why-both-r-and-python",
    "href": "posts/2025-02-21-ts-forecast/index.html#why-both-r-and-python",
    "title": "Time Series Forecasting: First Models in R and Python",
    "section": "",
    "text": "While Python has become my go-to language for data science work, I feel R is more intutive for certain aspects of time series forecasting due to the great packages which have been devolped. Each has distinct advantages:\n\nR excels with its purpose-built statistical packages and elegant handling of time series objects\nPython offers incredible flexibility, integration with other systems, and powerful machine learning capabilities\n\nLearning both approaches gives you a more complete toolkit and the ability to choose the right tool for each specific forecasting challenge."
  },
  {
    "objectID": "posts/2025-02-21-ts-forecast/index.html#getting-started-with-time-series-data",
    "href": "posts/2025-02-21-ts-forecast/index.html#getting-started-with-time-series-data",
    "title": "Time Series Forecasting: First Models in R and Python",
    "section": "",
    "text": "Before diving into models, we need data. For this example, let’s use a classic time series dataset: monthly airline passenger numbers. This dataset shows a clear upward trend and strong seasonality, making it perfect for demonstrating basic forecasting techniques."
  },
  {
    "objectID": "posts/2025-02-21-ts-forecast/index.html#first-models-in-r",
    "href": "posts/2025-02-21-ts-forecast/index.html#first-models-in-r",
    "title": "Time Series Forecasting: First Models in R and Python",
    "section": "",
    "text": "R has specialized libraries designed specifically for time series analysis. The forecast package, created by Rob Hyndman, is particularly powerful and user-friendly.\n\n\n# Install and load necessary packages\ninstall.packages(c(\"forecast\", \"tseries\"))\nlibrary(forecast)\nlibrary(tseries)\nlibrary(ggplot2)\n\n# Load the airlines dataset (included in R)\ndata(\"AirPassengers\")\nair &lt;- AirPassengers  # Monthly totals of airline passengers from 1949-1960\n\n# Examine the data\nclass(air)  # Should be \"ts\" (time series)\nfrequency(air)  # 12 = monthly data\nstart(air)  # Starting time period\nend(air)  # Ending time period\n\n# Plot the data\nautoplot(air) + \n  ggtitle(\"Monthly Airline Passenger Numbers 1949-1960\") + \n  xlab(\"Year\") + \n  ylab(\"Passengers (thousands)\")\n\n# Decompose the time series\nair_decomp &lt;- decompose(air, type = \"multiplicative\")\nautoplot(air_decomp)\n\n# Check stationarity with Augmented Dickey-Fuller test\nadf.test(air)  # p-value &gt; 0.05 indicates non-stationarity\n\n# Differencing to achieve stationarity\nair_diff &lt;- diff(log(air))\nadf.test(air_diff)  # Should now be stationary\n\n# Examine ACF and PACF for model parameter selection\nacf(air_diff, main=\"ACF of Differenced Log Passenger Data\")\npacf(air_diff, main=\"PACF of Differenced Log Passenger Data\")\n\n# Fit an ARIMA model automatically\nair_model &lt;- auto.arima(air, seasonal = TRUE)\nsummary(air_model)\n\n# Forecast the next 24 months\nair_forecast &lt;- forecast(air_model, h = 24)\n\n# Plot the forecast\nautoplot(air_forecast) + \n  ggtitle(\"24-Month Forecast of Airline Passengers\")\nThis R example walks through the complete process: 1. Loading and examining the data 2. Visualizing and decomposing the time series 3. Testing for stationarity (crucial for ARIMA models) 4. Using auto.arima() to automatically select the best parameters 5. Generating and visualizing a forecast\nThe auto.arima() function automatically determines the appropriate ARIMA model parameters (p, d, q) and their seasonal counterparts (P, D, Q), saving us from manual parameter tuning."
  },
  {
    "objectID": "posts/2025-02-21-ts-forecast/index.html#first-models-in-python",
    "href": "posts/2025-02-21-ts-forecast/index.html#first-models-in-python",
    "title": "Time Series Forecasting: First Models in R and Python",
    "section": "",
    "text": "Python offers multiple libraries for time series forecasting. The statsmodels package provides statistical models while pandas handles the data manipulation.\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom pandas.plotting import register_matplotlib_converters\nregister_matplotlib_converters()\n\n# Load the AirPassengers dataset\nair = pd.read_csv('https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv',\n                  index_col='Month', parse_dates=True)\nair.columns = ['Passengers']\n\n# Plot the data\nplt.figure(figsize=(10, 6))\nplt.plot(air)\nplt.title('Monthly Airline Passenger Numbers 1949-1960')\nplt.xlabel('Year')\nplt.ylabel('Passengers (thousands)')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# Decompose the time series\ndecomposition = seasonal_decompose(air, model='multiplicative')\nfig = decomposition.plot()\nfig.set_size_inches(10, 8)\nplt.tight_layout()\nplt.show()\n\n# Check stationarity with Augmented Dickey-Fuller test\ndef adf_test(series):\n    result = adfuller(series.dropna())\n    print('ADF Statistic: %f' % result[0])\n    print('p-value: %f' % result[1])\n    print('Critical Values:')\n    for key, value in result[4].items():\n        print('\\t%s: %.3f' % (key, value))\n    if result[1] &lt;= 0.05:\n        print(\"Conclusion: The series is stationary\")\n    else:\n        print(\"Conclusion: The series is non-stationary\")\n\nadf_test(air['Passengers'])\n\n# Take log and difference to make stationary\nair['LogPassengers'] = np.log(air['Passengers'])\nair['LogDiffPassengers'] = air['LogPassengers'].diff()\nadf_test(air['LogDiffPassengers'].dropna())\n\n# Plot ACF and PACF for parameter selection\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\nplot_acf(air['LogDiffPassengers'].dropna(), ax=ax1)\nplot_pacf(air['LogDiffPassengers'].dropna(), ax=ax2)\nplt.tight_layout()\nplt.show()\n\n# Fit a SARIMA model (with parameters based on ACF/PACF)\n# For this dataset, a SARIMA(1,1,1)(1,1,1,12) often works well\nmodel = SARIMAX(air['Passengers'], order=(1, 1, 1), seasonal_order=(1, 1, 1, 12))\nresults = model.fit()\nprint(results.summary())\n\n# Forecast the next 24 months\nforecast_steps = 24\nforecast_index = pd.date_range(start=air.index[-1], periods=forecast_steps+1, freq='MS')[1:]\nforecast = results.forecast(forecast_steps)\nforecast_df = pd.DataFrame(forecast, index=forecast_index, columns=['Forecast'])\n\n# Plot the actual data and the forecast\nplt.figure(figsize=(12, 6))\nplt.plot(air['Passengers'], label='Historical data')\nplt.plot(forecast_df, label='Forecast')\nplt.title('24-Month Forecast of Airline Passengers')\nplt.xlabel('Year')\nplt.ylabel('Passengers (thousands)')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# Calculate and plot confidence intervals\npred = results.get_prediction(start=air.index[-1], end=forecast_index[-1])\npred_conf = pred.conf_int()\n\nplt.figure(figsize=(12, 6))\nplt.plot(air['Passengers'], label='Historical data')\nplt.plot(pred.predicted_mean, label='Forecast')\nplt.fill_between(pred_conf.index, \n                 pred_conf.iloc[:, 0], \n                 pred_conf.iloc[:, 1], \n                 color='k', alpha=0.2)\nplt.title('24-Month Forecast with 95% Confidence Interval')\nplt.xlabel('Year')\nplt.ylabel('Passengers (thousands)')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\nThe Python example follows a similar workflow to the R version, but with Python-specific libraries:\n\nLoading data with pandas\nVisualizing with matplotlib\nDecomposing and testing stationarity\nFitting a SARIMA model using statsmodels\nForecasting future values and plotting with confidence intervals"
  },
  {
    "objectID": "posts/2025-02-21-ts-forecast/index.html#key-differences-between-r-and-python-approaches",
    "href": "posts/2025-02-21-ts-forecast/index.html#key-differences-between-r-and-python-approaches",
    "title": "Time Series Forecasting: First Models in R and Python",
    "section": "",
    "text": "As you can see, while the overall process is similar, there are some notable differences:\n\nData Handling: R has native support for time series objects, while Python relies on pandas DateTime indexes\nModel Selection: R’s auto.arima() automatically selects parameters, while in Python we typically need to analyze ACF/PACF plots or use grid search\nVisualization: R’s plotting is more concise with the forecast package, while Python offers more customization with matplotlib\nPerformance: For larger datasets, Python may offer better performance, especially when integrated with other machine learning workflows"
  },
  {
    "objectID": "posts/2025-02-21-ts-forecast/index.html#beyond-arima-next-steps",
    "href": "posts/2025-02-21-ts-forecast/index.html#beyond-arima-next-steps",
    "title": "Time Series Forecasting: First Models in R and Python",
    "section": "",
    "text": "While ARIMA and SARIMA models are great starting points, there are many other approaches to explore:\n\nExponential Smoothing: Simple yet powerful, especially for data with strong seasonality\nProphet: Facebook’s forecasting tool that handles holidays and events effectively\nLSTM Networks: Deep learning approaches for complex sequential patterns\nXGBoost with Lag Features: Machine learning with engineered time features\n\nI’ll cover these more advanced techniques in future posts, showing implementations in both R and Python where appropriate."
  },
  {
    "objectID": "posts/2025-02-21-ts-forecast/index.html#wrapping-up",
    "href": "posts/2025-02-21-ts-forecast/index.html#wrapping-up",
    "title": "Time Series Forecasting: First Models in R and Python",
    "section": "",
    "text": "These examples demonstrate basic forecasting models in R and Python, but they’re just the beginning. The choice between R and Python often comes down to your specific needs and existing workflow:\n\nChoose R if your work is primarily statistical and you value specialized time series tools\nChoose Python if you need integration with larger systems or want to leverage deep learning approaches\n\nIn my experience, learning both gives you the most flexibility. Many data scientists start with R for its statistical focus, then migrate to Python as they integrate forecasting into broader data pipelines and applications.\nIn the next post, I’ll dive deeper into model evaluation and selection - how to measure forecast accuracy and choose the right model for your specific time series problem."
  },
  {
    "objectID": "posts/2025-01-27-machine-learning-forecast/index.html",
    "href": "posts/2025-01-27-machine-learning-forecast/index.html",
    "title": "Time Series Forecasting: Understanding the Components",
    "section": "",
    "text": "Time series forecasting is a fascinating part of data science and machine learning. I first encountered it during a machine learning course where we briefly covered various models and techniques like classification, regression, random forests, LSTMs, CNNs, and gradient boosted models. In that class I was tasked with predicting bike sales. When I mentioned what I was learning to a family member, they asked if these techniques could be applied to demand forecasting in the context of their specific business. I thought it couldn’t be too different from what I did in class, and said yes. I quickly discovered the depth and complexity of time series forecasting was so much greater than what I had thought before.\nFor this post, I’ve use python to create several fake datasets to clearly demonstrate time series concepts without the noise and complexity of real-world data. While synthetic, these examples reflect patterns commonly found in actual time series data.\n\n\n\nComplete Series Example\n\n\nThis visualization shows a synthetic time series with trend, seasonality, and random variations. Even in this simplified example, we can observe several key characteristics of time series data:\n\nA clear upward trend over time\nRegular seasonal patterns that repeat at fixed intervals\nRandom variations that create “noise” around these patterns\n\n\n\nTime series forecasting is the process of analyzing historical time-ordered data to predict future values. Unlike other types of predictive modeling, time series forecasting explicitly accounts for the temporal ordering and relationships in the data.\nLife is full of patterns - we find them in nature, buildings, and data. Time series forecasting extracts these patterns from historical data to predict future outcomes. The defining characteristic is the time element: measurements recorded at consistent intervals, whether minutes, hours, days, weeks, or months. Consistency is crucial - if your data sometimes uses daily measurements and other times weekly, you must standardize to a single time unit before analysis.\n\n\n\nBefore you can effectively forecast a time series, you need to understand its components. Most time series can be broken down into four primary components:\n\n\nThe trend represents the long-term progression of your series - whether values are generally increasing, decreasing, or remaining stable over time. As shown in Figure 2, trends can take many forms.\nIdentifying the correct trend pattern is crucial for making accurate long-term forecasts.\n\n\n\nTrend Patterns\n\n\n\n\n\nSeasonality refers to repeating patterns that occur at regular intervals. As shown in Figure 3, these patterns can vary widely:\n\nAnnual patterns (think retail sales peaking during holidays)\nQuarterly patterns (often seen in financial data)\nMonthly, weekly, or daily cycles\nIrregular but predictable peaks (like special events)\nMultiple overlapping seasonal patterns\n\nIn retail data, for example, you might see weekly patterns (higher sales on weekends) overlaid with annual patterns (holiday shopping seasons).\n\n\n\nSeasonality Patterns\n\n\n\n\n\nCyclical patterns are longer-term fluctuations that don’t have a fixed frequency, unlike seasonality. Business cycles that affect performance over several years are a classic example, where boom and bust periods create waves in the data.\n\n\n\nAfter accounting for trend, seasonality, and cyclicity, what remains is the residual or noise component. This represents random, unpredictable variations that can’t be explained by the model. While true randomness can never be predicted, analyzing this component can provide insights into the volatility of your data and the reliability of your forecasts.\n\n\n\n\nTime series decomposition is the process of separating a time series into its component parts. This helps us understand the underlying patterns driving our data.\n\n\n\nStandard Decomposition\n\n\nIn Figure 5, we can see a time series broken down into its components: - The observed data (top panel) - The extracted trend (second panel) - The seasonal pattern (third panel) - The residual noise (bottom panel)\nThis decomposition follows an additive model, represented by the formula:\n\\[Y_t = T_t + S_t + C_t + R_t\\]\nWhere: - \\(Y_t\\) is the observed value at time \\(t\\)  - \\(T_t\\) is the trend component - \\(S_t\\) is the seasonal component - \\(C_t\\) is the cyclical component - \\(R_t\\) is the residual (noise) component\n\n\n\nThere are two main approaches to time series decomposition: additive and multiplicative.\n\n\n\nAdditive vs Multiplicative Series\n\n\n\n\nIn an additive model, the components are added together to form the original series:\n\\[Y_t = T_t + S_t + C_t + R_t\\]\nThis model is appropriate when: - The seasonal variations add a fixed amount regardless of the trend level - For example, ice cream sales might consistently increase by exactly 1,000 units during summer months, whether your baseline is 3,000 or 8,000 units - The data can have negative values (since multiplication by negative values can produce counterintuitive results)\nIn Figure 6 (top panel), you can see that the seasonal variations maintain a consistent amplitude regardless of the trend level.\n\n\n\nIn a multiplicative model, the components are multiplied together:\n\\[Y_t = T_t \\times S_t \\times C_t \\times R_t\\]\nThis model is appropriate when: - The seasonal variations represent a percentage of the trend level - For example, retail sales might increase by 40% during December, meaning a much larger absolute increase when baseline sales are higher - The data must be positive (since multiplication by zero or negative values distorts the pattern)\nIn Figure 6 (bottom panel), note how the seasonal variations grow larger as the trend increases.\n\n\n\nMultiplicative Decomposition\n\n\nFigure 7 shows the decomposition of a multiplicative series. Notice how the seasonal component is expressed as a ratio (values around 1.0) rather than absolute values.\n\n\n\n\nThe MOST important step when approaching time series forecasting is researching the domain your data comes from. If you’re working with sales data from a janitorial products warehouse, take time to understand that industry. Talk to people working in it to discover the key factors driving their sales. This domain knowledge will be invaluable as you build your models and interpret results.\nDomain knowledge helps you:\n\nIdentify expected patterns - Industry experts can tell you about normal seasonal fluctuations\nExplain anomalies - That unexplained spike might be due to a one-time event\nUnderstand structural changes - A sudden shift in the data might be explained by a change in business strategy or market conditions\nSelect appropriate models - Different types of data require different modeling approaches\n\nWithout domain context, you risk misinterpreting the data or making incorrect forecasts. When you combine solid statistical techniques with domain understanding, time series forecasting becomes a powerful tool for business planning, resource usage, and strategic decision-making.\n\n\n\nIn practice, time series data is rarely as clean as our simple decomposition models suggest. Figure 4 highlights several real-world complications:\n\nOutliers: Unusual events can create extreme values (shown in the middle-right panel)\nMissing values: Data collection issues can lead to gaps in your time series\nLevel shifts: Structural changes can cause permanent shifts in the level of your data\nHeteroscedasticity: The variance of your data might change over time, making some periods more volatile than others\n\nAddressing these complications is an important part of time series forecasting, and will be covered in future posts.\n\n\n\nIn this post I introduced the foundational concepts of time series forecasting, including the components that make up a time series and the different approaches to decomposition. Understanding these concepts is crucial before diving into the practical implementation of forecasting models.\nIn my next post, I’ll explore how to implement these concepts in code, working with real-world datasets to build and evaluate forecasting models."
  },
  {
    "objectID": "posts/2025-01-27-machine-learning-forecast/index.html#before-you-start-coding",
    "href": "posts/2025-01-27-machine-learning-forecast/index.html#before-you-start-coding",
    "title": "Time Series Forecasting: Uncovering Patterns in Your Data",
    "section": "",
    "text": "The MOST important step when approaching time series forecasting is researching the domain your data comes from. If you’re working with sales data from a janitorial products warehouse, take time to understand that industry. Talk to people working in it to discover the key factors driving their sales. This domain knowledge will be invaluable as you build your models and interpret results."
  },
  {
    "objectID": "posts/2025-01-27-machine-learning-forecast/index.html#the-basics",
    "href": "posts/2025-01-27-machine-learning-forecast/index.html#the-basics",
    "title": "Time Series Forecasting: Uncovering Patterns in Your Data",
    "section": "",
    "text": "What exactly is time series forecasting? Life is full of patterns - we find them in nature, buildings, and data. Time series forecasting extracts patterns from historical data to predict future outcomes. The defining characteristic is the time element: measurements recorded at consistent intervals, whether minutes, hours, days, weeks, or months. Consistency is crucial - if your data sometimes uses daily measurements and other times weekly, you must standardize to a single time unit before analysis."
  },
  {
    "objectID": "posts/2025-01-27-machine-learning-forecast/index.html#extracting-the-patterns",
    "href": "posts/2025-01-27-machine-learning-forecast/index.html#extracting-the-patterns",
    "title": "Time Series Forecasting: Uncovering Patterns in Your Data",
    "section": "",
    "text": "Once your data is properly structured with consistent time intervals, you can begin identifying patterns and separating the data into its different components through a process called decomposition. Time series data typically contains four main components:\n\n\n\nDaily Energy Consumption Pattern\n\n\nThis visualization reveals one of the most fundamental patterns in energy consumption data: the daily cycle. Notice how energy use drops to its lowest point around 4-5 AM when most people are sleeping, then steadily rises throughout the morning. Consumption peaks in the late afternoon and early evening (around 6-8 PM) before declining again as night falls. This distinctive pattern represents the seasonality component I discussed earlier - a predictable cycle that repeats at regular intervals.\n\n\nThe long-term progression of your series - whether values are generally increasing, decreasing, or remaining stable over time. Trends represent the underlying direction of your data.\n\n\n\nRepeating patterns that occur at regular intervals. These might be daily, weekly, monthly, or yearly cycles depending on your data. Holiday shopping spikes in retail or specific times of year being known for having more storms are examples of seasonality.\n\n\n\nLonger-term patterns that don’t have a fixed frequency. Economic cycles that affect business performance over several years are a classic example, where boom and bust periods create waves in the data.\n\n\n\nThe random variation remaining after accounting for trend, seasonality, and cyclicity. This represents unpredictable fluctuations that can’t be explained by the model.\nThese components combine (either additively or multiplicatively) to create the overall time series pattern.\n\n\n\nTime Series Decomposition of Energy Consumption Data\n\n\nHere we can see a practical application of time series decomposition. The top panel shows the original observed data spanning from 2002 to 2018. Below it, we can see:\n\nTrend Component: The second panel reveals the long-term movement in energy demand, with seasonal fluctuations removed.\nSeasonality Component: The third panel shows regular, predictable patterns that repeat over time.\nResiduals: The bottom panel shows what remains after removing trend and seasonality - the random variation or “noise” that can’t be explained by our model.\n\nThis decomposition is invaluable for understanding the underlying drivers of energy consumption and creating more accurate forecasts."
  },
  {
    "objectID": "posts/2025-01-27-machine-learning-forecast/index.html#understanding-the-patterns",
    "href": "posts/2025-01-27-machine-learning-forecast/index.html#understanding-the-patterns",
    "title": "Time Series Forecasting: Uncovering Patterns in Your Data",
    "section": "",
    "text": "After decomposing your time series, you can use this information to make better predictions. Trends and seasonal patterns often become very noticeable - perhaps sales peak annually during the holiday season or just before school starts.\nThis is why domain knowledge is so critical. Understanding external factors like natural disasters, company sales strategies, or market conditions helps explain outliers, spikes, or dips that might otherwise confuse your model. Without this context, you risk misinterpreting the data or making incorrect forecasts.\nWhen you combine solid statistical techniques with domain understanding, time series forecasting becomes a powerful tool for business planning, resource usage, and strategic decision-making."
  },
  {
    "objectID": "posts/2025-01-27-machine-learning-forecast/index.html#looking-ahead",
    "href": "posts/2025-01-27-machine-learning-forecast/index.html#looking-ahead",
    "title": "Time Series Forecasting: Understanding the Components",
    "section": "",
    "text": "In this post I introduced the foundational concepts of time series forecasting, including the components that make up a time series and the different approaches to decomposition. Understanding these concepts is crucial before diving into the practical implementation of forecasting models.\nIn my next post, I’ll explore how to implement these concepts in code, working with real-world datasets to build and evaluate forecasting models."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Tyler Lowry",
    "section": "About Me",
    "text": "About Me\nAt BYU-Idaho, I serve as both a TA/Tutor in the Mathematics Department and Team Lead at the McKay Maclab, which has strengthened my skills in programming, statistics, and leadership.\nAfter graduating in April 2025, I’ll be continuing my education at Texas A&M in the Statistics Department for a Master’s in Statistical Data Science."
  },
  {
    "objectID": "index.html#featured-projects",
    "href": "index.html#featured-projects",
    "title": "Tyler Lowry",
    "section": "",
    "text": "Check out my Projects page for a showcase of my recent work"
  },
  {
    "objectID": "resume.html#technical-skills",
    "href": "resume.html#technical-skills",
    "title": "Resume",
    "section": "Technical Skills",
    "text": "Technical Skills\n\n\n\nData Science\n\nPython (Pandas, Polars), R (Tidyverse)\nMachine Learning (TensorFlow, Scikit-Learn), Demand Forecasting\nVisualization (Tableau, Power BI, ggplot2)\n\n\n\n\nDevelopment & Engineering\n\nHTML, CSS, JavaScript, TypeScript, PHP, C#\nAPIs, ETL Pipelines, PySpark, SQL\nExcel/VBA, Adobe Suite, Microsoft Office"
  },
  {
    "objectID": "resume.html#tyler-lowry",
    "href": "resume.html#tyler-lowry",
    "title": "Resume",
    "section": "",
    "text": "tjlowry02@gmail.com | LinkedIn"
  },
  {
    "objectID": "posts/2025-01-27-machine-learning-forecast/index.html#what-exactly-is-time-series-forecasting",
    "href": "posts/2025-01-27-machine-learning-forecast/index.html#what-exactly-is-time-series-forecasting",
    "title": "Time Series Forecasting: Understanding the Components",
    "section": "",
    "text": "Time series forecasting is the process of analyzing historical time-ordered data to predict future values. Unlike other types of predictive modeling, time series forecasting explicitly accounts for the temporal ordering and relationships in the data.\nLife is full of patterns - we find them in nature, buildings, and data. Time series forecasting extracts these patterns from historical data to predict future outcomes. The defining characteristic is the time element: measurements recorded at consistent intervals, whether minutes, hours, days, weeks, or months. Consistency is crucial - if your data sometimes uses daily measurements and other times weekly, you must standardize to a single time unit before analysis."
  },
  {
    "objectID": "posts/2025-01-27-machine-learning-forecast/index.html#understanding-time-series-components",
    "href": "posts/2025-01-27-machine-learning-forecast/index.html#understanding-time-series-components",
    "title": "Time Series Forecasting: Understanding the Components",
    "section": "",
    "text": "Before you can effectively forecast a time series, you need to understand its components. Most time series can be broken down into four primary components:\n\n\nThe trend represents the long-term progression of your series - whether values are generally increasing, decreasing, or remaining stable over time. As shown in Figure 2, trends can take many forms.\nIdentifying the correct trend pattern is crucial for making accurate long-term forecasts.\n\n\n\nTrend Patterns\n\n\n\n\n\nSeasonality refers to repeating patterns that occur at regular intervals. As shown in Figure 3, these patterns can vary widely:\n\nAnnual patterns (think retail sales peaking during holidays)\nQuarterly patterns (often seen in financial data)\nMonthly, weekly, or daily cycles\nIrregular but predictable peaks (like special events)\nMultiple overlapping seasonal patterns\n\nIn retail data, for example, you might see weekly patterns (higher sales on weekends) overlaid with annual patterns (holiday shopping seasons).\n\n\n\nSeasonality Patterns\n\n\n\n\n\nCyclical patterns are longer-term fluctuations that don’t have a fixed frequency, unlike seasonality. Business cycles that affect performance over several years are a classic example, where boom and bust periods create waves in the data.\n\n\n\nAfter accounting for trend, seasonality, and cyclicity, what remains is the residual or noise component. This represents random, unpredictable variations that can’t be explained by the model. While true randomness can never be predicted, analyzing this component can provide insights into the volatility of your data and the reliability of your forecasts."
  },
  {
    "objectID": "posts/2025-01-27-machine-learning-forecast/index.html#decomposing-a-time-series",
    "href": "posts/2025-01-27-machine-learning-forecast/index.html#decomposing-a-time-series",
    "title": "Time Series Forecasting: Understanding the Components",
    "section": "",
    "text": "Time series decomposition is the process of separating a time series into its component parts. This helps us understand the underlying patterns driving our data.\n\n\n\nStandard Decomposition\n\n\nIn Figure 5, we can see a time series broken down into its components: - The observed data (top panel) - The extracted trend (second panel) - The seasonal pattern (third panel) - The residual noise (bottom panel)\nThis decomposition follows an additive model, represented by the formula:\n\\[Y_t = T_t + S_t + C_t + R_t\\]\nWhere: - \\(Y_t\\) is the observed value at time \\(t\\)  - \\(T_t\\) is the trend component - \\(S_t\\) is the seasonal component - \\(C_t\\) is the cyclical component - \\(R_t\\) is the residual (noise) component"
  },
  {
    "objectID": "posts/2025-01-27-machine-learning-forecast/index.html#additive-vs.-multiplicative-decomposition",
    "href": "posts/2025-01-27-machine-learning-forecast/index.html#additive-vs.-multiplicative-decomposition",
    "title": "Time Series Forecasting: Understanding the Components",
    "section": "",
    "text": "There are two main approaches to time series decomposition: additive and multiplicative.\n\n\n\nAdditive vs Multiplicative Series\n\n\n\n\nIn an additive model, the components are added together to form the original series:\n\\[Y_t = T_t + S_t + C_t + R_t\\]\nThis model is appropriate when: - The seasonal variations add a fixed amount regardless of the trend level - For example, ice cream sales might consistently increase by exactly 1,000 units during summer months, whether your baseline is 3,000 or 8,000 units - The data can have negative values (since multiplication by negative values can produce counterintuitive results)\nIn Figure 6 (top panel), you can see that the seasonal variations maintain a consistent amplitude regardless of the trend level.\n\n\n\nIn a multiplicative model, the components are multiplied together:\n\\[Y_t = T_t \\times S_t \\times C_t \\times R_t\\]\nThis model is appropriate when: - The seasonal variations represent a percentage of the trend level - For example, retail sales might increase by 40% during December, meaning a much larger absolute increase when baseline sales are higher - The data must be positive (since multiplication by zero or negative values distorts the pattern)\nIn Figure 6 (bottom panel), note how the seasonal variations grow larger as the trend increases.\n\n\n\nMultiplicative Decomposition\n\n\nFigure 7 shows the decomposition of a multiplicative series. Notice how the seasonal component is expressed as a ratio (values around 1.0) rather than absolute values."
  },
  {
    "objectID": "posts/2025-01-27-machine-learning-forecast/index.html#the-critical-role-of-domain-knowledge",
    "href": "posts/2025-01-27-machine-learning-forecast/index.html#the-critical-role-of-domain-knowledge",
    "title": "Time Series Forecasting: Understanding the Components",
    "section": "",
    "text": "The MOST important step when approaching time series forecasting is researching the domain your data comes from. If you’re working with sales data from a janitorial products warehouse, take time to understand that industry. Talk to people working in it to discover the key factors driving their sales. This domain knowledge will be invaluable as you build your models and interpret results.\nDomain knowledge helps you:\n\nIdentify expected patterns - Industry experts can tell you about normal seasonal fluctuations\nExplain anomalies - That unexplained spike might be due to a one-time event\nUnderstand structural changes - A sudden shift in the data might be explained by a change in business strategy or market conditions\nSelect appropriate models - Different types of data require different modeling approaches\n\nWithout domain context, you risk misinterpreting the data or making incorrect forecasts. When you combine solid statistical techniques with domain understanding, time series forecasting becomes a powerful tool for business planning, resource usage, and strategic decision-making."
  },
  {
    "objectID": "posts/2025-01-27-machine-learning-forecast/index.html#real-world-complications",
    "href": "posts/2025-01-27-machine-learning-forecast/index.html#real-world-complications",
    "title": "Time Series Forecasting: Understanding the Components",
    "section": "",
    "text": "In practice, time series data is rarely as clean as our simple decomposition models suggest. Figure 4 highlights several real-world complications:\n\nOutliers: Unusual events can create extreme values (shown in the middle-right panel)\nMissing values: Data collection issues can lead to gaps in your time series\nLevel shifts: Structural changes can cause permanent shifts in the level of your data\nHeteroscedasticity: The variance of your data might change over time, making some periods more volatile than others\n\nAddressing these complications is an important part of time series forecasting, and will be covered in future posts."
  }
]